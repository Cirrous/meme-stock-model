{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c836917d",
   "metadata": {},
   "source": [
    "# Tesla Stock Prediction from Reddit Sentiment\n",
    "\n",
    "Dieses Notebook trainiert ein Machine Learning Modell, um basierend auf Reddit Posts √ºber Tesla zu prognostizieren, ob der Aktienkurs steigt oder f√§llt.\n",
    "\n",
    "## Workflow:\n",
    "1. Daten laden (Reddit Posts + Tesla Kursdaten)\n",
    "2. Labels erstellen (Kurs steigt/f√§llt nach Post)\n",
    "3. Features vorbereiten (Embeddings + Sentiment)\n",
    "4. Modell trainieren\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000e8bb",
   "metadata": {},
   "source": [
    "## 1. Import Libraries und Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Alle Libraries erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbfadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombinierte Daten mit Embeddings, Sentiment und Upvotes laden\n",
    "try:\n",
    "    # Lade die komplette Datei mit allem drin\n",
    "    data = np.load('data/complet_embedding.npy', allow_pickle=True)\n",
    "    \n",
    "    # Falls es ein Dictionary ist, extrahiere die Komponenten\n",
    "    if isinstance(data.item(), dict):\n",
    "        data_dict = data.item()\n",
    "        embeddings = data_dict.get('embeddings')\n",
    "        reddit_data = data_dict.get('reddit_data')  # Sentiment, Text, Upvotes, Datum\n",
    "        stock_data = data_dict.get('stock_data', None)  # Optional: Kursdaten\n",
    "        \n",
    "        print(f\"üìä Kombinierte Daten geladen:\")\n",
    "        print(f\"   üß† Embeddings: {embeddings.shape if embeddings is not None else 'Keine'}\")\n",
    "        print(f\"   üìù Reddit Posts: {len(reddit_data)} Eintr√§ge\")\n",
    "        if stock_data is not None:\n",
    "            print(f\"   üìà Kursdaten: {len(stock_data)} Eintr√§ge\")\n",
    "    else:\n",
    "        # Falls es nur ein Array ist, nehmen wir an dass es nur Embeddings sind\n",
    "        embeddings = data\n",
    "        reddit_data = None\n",
    "        stock_data = None\n",
    "        print(f\"üß† Nur Embeddings geladen: {embeddings.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fehler beim Laden von complet_embedding.npy: {e}\")\n",
    "    print(\"üìã Erstelle Mock-Daten f√ºr Demo...\")\n",
    "    \n",
    "    # Fallback: Mock-Daten erstellen\n",
    "    n_samples = 1000\n",
    "    embedding_dim = 1024\n",
    "    \n",
    "    embeddings = np.random.randn(n_samples, embedding_dim)\n",
    "    \n",
    "    # Mock Reddit-Daten\n",
    "    reddit_data = {\n",
    "        'text': [f\"Tesla post {i}\" for i in range(n_samples)],\n",
    "        'finbert_sentiment': np.random.uniform(-1, 1, n_samples),\n",
    "        'vader_sentiment': np.random.uniform(-1, 1, n_samples),\n",
    "        'upvotes': np.random.randint(1, 100, n_samples),\n",
    "        'created': pd.date_range(start='2012-01-01', periods=n_samples, freq='H')\n",
    "    }\n",
    "    \n",
    "    stock_data = None\n",
    "    print(f\"‚úÖ Mock-Daten erstellt: {n_samples} Posts mit {embedding_dim}D Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09172853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten√ºbersicht der kombinierten Daten\n",
    "print(\"\\n=== KOMBINIERTE DATEN √úBERSICHT ===\")\n",
    "\n",
    "if reddit_data is not None:\n",
    "    # Falls Reddit-Daten als Dictionary vorliegen\n",
    "    if isinstance(reddit_data, dict):\n",
    "        reddit_df = pd.DataFrame(reddit_data)\n",
    "    else:\n",
    "        # Falls es schon ein DataFrame ist\n",
    "        reddit_df = reddit_data\n",
    "        \n",
    "    print(\"Reddit Posts:\")\n",
    "    print(reddit_df.head())\n",
    "    print(f\"\\nSpalten: {list(reddit_df.columns)}\")\n",
    "    \n",
    "    if 'created' in reddit_df.columns:\n",
    "        print(f\"Datum-Range: {reddit_df['created'].min()} bis {reddit_df['created'].max()}\")\n",
    "    \n",
    "    if 'finbert_sentiment' in reddit_df.columns:\n",
    "        print(f\"FinBERT Sentiment Range: {reddit_df['finbert_sentiment'].min():.3f} bis {reddit_df['finbert_sentiment'].max():.3f}\")\n",
    "    \n",
    "    if 'upvotes' in reddit_df.columns:\n",
    "        print(f\"Upvotes Range: {reddit_df['upvotes'].min()} bis {reddit_df['upvotes'].max()}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Keine separaten Reddit-Daten verf√ºgbar\")\n",
    "    # Erstelle minimales DataFrame f√ºr Mock-Daten\n",
    "    reddit_df = pd.DataFrame({\n",
    "        'text': [f\"Mock Tesla post {i}\" for i in range(len(embeddings))],\n",
    "        'finbert_sentiment': np.random.uniform(-1, 1, len(embeddings)),\n",
    "        'vader_sentiment': np.random.uniform(-1, 1, len(embeddings)),\n",
    "        'upvotes': np.random.randint(1, 100, len(embeddings)),\n",
    "        'created': pd.date_range(start='2012-01-01', periods=len(embeddings), freq='H')\n",
    "    })\n",
    "    print(\"üìù Mock Reddit DataFrame erstellt\")\n",
    "\n",
    "if stock_data is not None:\n",
    "    print(\"\\n=== TESLA KURSDATEN √úBERSICHT ===\")\n",
    "    if isinstance(stock_data, dict):\n",
    "        stock_df = pd.DataFrame(stock_data)\n",
    "    else:\n",
    "        stock_df = stock_data\n",
    "    print(stock_df.head())\n",
    "    print(f\"\\nSpalten: {list(stock_df.columns)}\")\n",
    "else:\n",
    "    stock_df = None\n",
    "\n",
    "print(f\"\\nüß† Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"üìä Reddit Posts: {len(reddit_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dabee68",
   "metadata": {},
   "source": [
    "## 2. Datenvorverarbeitung und Label-Erstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cfe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit Posts vorbereiten\n",
    "reddit_df['created'] = pd.to_datetime(reddit_df['created'])\n",
    "reddit_df['date'] = reddit_df['created'].dt.date\n",
    "\n",
    "# Sortieren nach Datum\n",
    "reddit_df = reddit_df.sort_values('created').reset_index(drop=True)\n",
    "\n",
    "print(f\"Reddit Posts von {reddit_df['date'].min()} bis {reddit_df['date'].max()}\")\n",
    "\n",
    "# √úberpr√ºfe verf√ºgbare Sentiment-Spalten\n",
    "sentiment_cols = []\n",
    "if 'finbert_sentiment' in reddit_df.columns:\n",
    "    sentiment_cols.append('finbert_sentiment')\n",
    "    print(f\"FinBERT Sentiment Range: {reddit_df['finbert_sentiment'].min():.3f} bis {reddit_df['finbert_sentiment'].max():.3f}\")\n",
    "\n",
    "if 'vader_sentiment' in reddit_df.columns:\n",
    "    sentiment_cols.append('vader_sentiment')\n",
    "    print(f\"VADER Sentiment Range: {reddit_df['vader_sentiment'].min():.3f} bis {reddit_df['vader_sentiment'].max():.3f}\")\n",
    "\n",
    "if 'upvotes' in reddit_df.columns:\n",
    "    print(f\"Upvotes Range: {reddit_df['upvotes'].min()} bis {reddit_df['upvotes'].max()}\")\n",
    "\n",
    "print(f\"üìä Verf√ºgbare Sentiment-Features: {sentiment_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5103997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls keine echten Kursdaten vorhanden sind, erstellen wir Mock-Daten\n",
    "if stock_df is None:\n",
    "    print(\"‚ö†Ô∏è Erstelle Mock Tesla-Kursdaten f√ºr Demo...\")\n",
    "    \n",
    "    # Erstelle realistische Kursdaten basierend auf dem Zeitraum der Reddit Posts\n",
    "    start_date = reddit_df['date'].min()\n",
    "    end_date = reddit_df['date'].max()\n",
    "    \n",
    "    # Erstelle t√§gliche Daten\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    \n",
    "    # Simuliere Tesla-Kursdaten (basierend auf historischen Mustern)\n",
    "    np.random.seed(42)\n",
    "    base_price = 100  # Startpreis\n",
    "    prices = [base_price]\n",
    "    \n",
    "    for i in range(1, len(date_range)):\n",
    "        # Random Walk mit leichtem Aufw√§rtstrend\n",
    "        change = np.random.normal(0.001, 0.03)  # 0.1% Trend, 3% Volatilit√§t\n",
    "        new_price = prices[-1] * (1 + change)\n",
    "        prices.append(max(new_price, 10))  # Mindestpreis 10$\n",
    "    \n",
    "    stock_df = pd.DataFrame({\n",
    "        'date': date_range,\n",
    "        'close': prices,\n",
    "        'open': [p * (1 + np.random.normal(0, 0.01)) for p in prices],\n",
    "        'high': [p * (1 + abs(np.random.normal(0, 0.02))) for p in prices],\n",
    "        'low': [p * (1 - abs(np.random.normal(0, 0.02))) for p in prices]\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Mock-Kursdaten erstellt: {len(stock_df)} Tage\")\n",
    "else:\n",
    "    # Echte Kursdaten verarbeiten\n",
    "    # Annahme: erste Spalte ist Datum, dann OHLC\n",
    "    if 'date' not in stock_df.columns:\n",
    "        date_col = stock_df.columns[0]\n",
    "        stock_df['date'] = pd.to_datetime(stock_df[date_col])\n",
    "    else:\n",
    "        stock_df['date'] = pd.to_datetime(stock_df['date'])\n",
    "\n",
    "stock_df['date'] = stock_df['date'].dt.date\n",
    "stock_df = stock_df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f\"üìà Tesla Kursdaten: {stock_df['date'].min()} bis {stock_df['date'].max()}\")\n",
    "print(stock_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66044608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels erstellen: Steigt der Kurs X Tage nach dem Reddit Post?\n",
    "PREDICTION_DAYS = 3  # Vorhersage f√ºr 3 Tage in die Zukunft\n",
    "\n",
    "def create_labels(reddit_df, stock_df, prediction_days=3):\n",
    "    \"\"\"\n",
    "    Erstellt Labels f√ºr jeden Reddit Post:\n",
    "    1 = Kurs steigt in den n√§chsten X Tagen\n",
    "    0 = Kurs f√§llt oder bleibt gleich\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    # Stock-Daten als Dict f√ºr schnellen Zugriff\n",
    "    stock_dict = dict(zip(stock_df['date'], stock_df['close']))\n",
    "    \n",
    "    for idx, row in reddit_df.iterrows():\n",
    "        post_date = row['date']\n",
    "        future_date = post_date + timedelta(days=prediction_days)\n",
    "        \n",
    "        # Preis am Tag des Posts\n",
    "        current_price = stock_dict.get(post_date)\n",
    "        \n",
    "        # Preis X Tage sp√§ter\n",
    "        future_price = stock_dict.get(future_date)\n",
    "        \n",
    "        # Falls beide Preise verf√ºgbar sind\n",
    "        if current_price is not None and future_price is not None:\n",
    "            # Label: 1 wenn Kurs steigt, 0 wenn f√§llt/gleich\n",
    "            label = 1 if future_price > current_price else 0\n",
    "            labels.append(label)\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    return labels, valid_indices\n",
    "\n",
    "# Labels erstellen\n",
    "labels, valid_indices = create_labels(reddit_df, stock_df, PREDICTION_DAYS)\n",
    "\n",
    "print(f\"‚úÖ Labels erstellt f√ºr {len(labels)} Posts\")\n",
    "print(f\"üìä Verteilung - Steigt: {sum(labels)} ({sum(labels)/len(labels)*100:.1f}%), F√§llt: {len(labels)-sum(labels)} ({(len(labels)-sum(labels))/len(labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset f√ºr Training vorbereiten\n",
    "train_reddit = reddit_df.iloc[valid_indices].copy()\n",
    "train_labels = np.array(labels)\n",
    "\n",
    "# Embeddings entsprechend filtern\n",
    "train_embeddings = embeddings[valid_indices]\n",
    "print(f\"üß† Embeddings gefiltert: {train_embeddings.shape}\")\n",
    "\n",
    "print(f\"üìù Trainingsdaten: {len(train_reddit)} Samples\")\n",
    "print(f\"üìä Features: Embeddings + Sentiment + Upvotes + Zeit-Features\")\n",
    "print(f\"üéØ Labels: {len(train_labels)} (Kurs steigt/f√§llt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b67d90",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features zusammenstellen\n",
    "def prepare_features(reddit_df, embeddings):\n",
    "    \"\"\"\n",
    "    Bereitet Features f√ºr das Machine Learning vor\n",
    "    Kombiniert Embeddings mit zus√§tzlichen Meta-Features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # 1. Text Embeddings (Hauptfeatures)\n",
    "    features.append(embeddings)\n",
    "    feature_names.extend([f'embed_{i}' for i in range(embeddings.shape[1])])\n",
    "    \n",
    "    # 2. Sentiment Features (falls verf√ºgbar)\n",
    "    sentiment_features = []\n",
    "    if 'finbert_sentiment' in reddit_df.columns:\n",
    "        sentiment_features.append(reddit_df['finbert_sentiment'].values)\n",
    "        feature_names.append('finbert_sentiment')\n",
    "    \n",
    "    if 'vader_sentiment' in reddit_df.columns:\n",
    "        sentiment_features.append(reddit_df['vader_sentiment'].values)\n",
    "        feature_names.append('vader_sentiment')\n",
    "    \n",
    "    if len(sentiment_features) > 0:\n",
    "        sentiment_array = np.column_stack(sentiment_features)\n",
    "        features.append(sentiment_array)\n",
    "    \n",
    "    # 3. Engagement Features\n",
    "    if 'upvotes' in reddit_df.columns:\n",
    "        upvotes = reddit_df['upvotes'].values.reshape(-1, 1)\n",
    "        features.append(upvotes)\n",
    "        feature_names.append('upvotes')\n",
    "    elif 'score' in reddit_df.columns:\n",
    "        # Fallback auf 'score' falls 'upvotes' nicht existiert\n",
    "        score = reddit_df['score'].values.reshape(-1, 1)\n",
    "        features.append(score)\n",
    "        feature_names.append('reddit_score')\n",
    "    \n",
    "    # 4. Text-basierte Features\n",
    "    if 'text' in reddit_df.columns:\n",
    "        text_length = reddit_df['text'].str.len().values.reshape(-1, 1)\n",
    "        features.append(text_length)\n",
    "        feature_names.append('text_length')\n",
    "    \n",
    "    # 5. Zeitbasierte Features\n",
    "    reddit_df['hour'] = pd.to_datetime(reddit_df['created']).dt.hour\n",
    "    reddit_df['weekday'] = pd.to_datetime(reddit_df['created']).dt.weekday\n",
    "    time_features = np.column_stack([\n",
    "        reddit_df['hour'].values,\n",
    "        reddit_df['weekday'].values\n",
    "    ])\n",
    "    features.append(time_features)\n",
    "    feature_names.extend(['hour', 'weekday'])\n",
    "    \n",
    "    # Alle Features kombinieren\n",
    "    X = np.concatenate(features, axis=1)\n",
    "    \n",
    "    return X, feature_names\n",
    "\n",
    "# Features erstellen\n",
    "X, feature_names = prepare_features(train_reddit, train_embeddings)\n",
    "y = train_labels\n",
    "\n",
    "print(f\"‚úÖ Features erstellt: {X.shape}\")\n",
    "print(f\"üìä Feature-Aufbau:\")\n",
    "print(f\"   ‚Ä¢ Embeddings: {train_embeddings.shape[1]}D\")\n",
    "print(f\"   ‚Ä¢ Meta-Features: {len(feature_names) - train_embeddings.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Gesamt: {len(feature_names)} Features\")\n",
    "print(f\"üéØ Labels: {y.shape}, Positive Klasse: {y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846acbf",
   "metadata": {},
   "source": [
    "## 4. Modell Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Training: {X_train.shape[0]} Samples\")\n",
    "print(f\"üìä Test: {X_test.shape[0]} Samples\")\n",
    "\n",
    "# Features skalieren\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features skaliert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verschiedene Modelle trainieren\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüöÄ Trainiere {name}...\")\n",
    "    \n",
    "    # Training\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_prob\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name}: Accuracy = {accuracy:.3f}, ROC-AUC = {roc_auc:.3f}\")\n",
    "\n",
    "print(\"\\nüèÜ Training abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796fda7",
   "metadata": {},
   "source": [
    "## 5. Evaluation und Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detaillierte Evaluation\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä EVALUATION: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    print(f\"Accuracy: {result['accuracy']:.3f}\")\n",
    "    print(f\"ROC-AUC: {result['roc_auc']:.3f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, result['predictions']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierungen\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Tesla Stock Prediction - Model Evaluation', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Modell-Vergleich\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "roc_aucs = [results[name]['roc_auc'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,0].bar(x - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "axes[0,0].bar(x + width/2, roc_aucs, width, label='ROC-AUC', alpha=0.8)\n",
    "axes[0,0].set_xlabel('Models')\n",
    "axes[0,0].set_ylabel('Score')\n",
    "axes[0,0].set_title('Model Performance Comparison')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(model_names)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Confusion Matrix (bestes Modell)\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['roc_auc'])\n",
    "cm = confusion_matrix(y_test, results[best_model_name]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "axes[0,1].set_xlabel('Predicted')\n",
    "axes[0,1].set_ylabel('Actual')\n",
    "\n",
    "# 3. Sentiment vs. Predictions\n",
    "test_indices = X_test[:, 0]  # FinBERT Sentiment (erstes Feature)\n",
    "colors = ['red' if pred == 1 else 'blue' for pred in results[best_model_name]['predictions']]\n",
    "axes[1,0].scatter(test_indices, results[best_model_name]['probabilities'], c=colors, alpha=0.6)\n",
    "axes[1,0].set_xlabel('FinBERT Sentiment')\n",
    "axes[1,0].set_ylabel('Prediction Probability')\n",
    "axes[1,0].set_title('Sentiment vs. Prediction Probability')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature Importance (falls Random Forest das beste Modell ist)\n",
    "if best_model_name == 'Random Forest':\n",
    "    importances = results[best_model_name]['model'].feature_importances_\n",
    "    # Top 10 wichtigste Features\n",
    "    top_indices = np.argsort(importances)[-10:]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    top_importances = importances[top_indices]\n",
    "    \n",
    "    axes[1,1].barh(range(len(top_features)), top_importances)\n",
    "    axes[1,1].set_yticks(range(len(top_features)))\n",
    "    axes[1,1].set_yticklabels(top_features)\n",
    "    axes[1,1].set_xlabel('Feature Importance')\n",
    "    axes[1,1].set_title('Top 10 Most Important Features')\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Feature Importance\\nnur f√ºr Random Forest\\nverf√ºgbar', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüèÜ Bestes Modell: {best_model_name} (ROC-AUC: {results[best_model_name]['roc_auc']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178f1fb",
   "metadata": {},
   "source": [
    "## 6. Prediction Function f√ºr neue Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ee84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_stock_movement(text, sentiment_finbert=None, sentiment_vader=None, upvotes=1, \n",
    "                          model_name='best', embedding=None):\n",
    "    \"\"\"\n",
    "    Prognostiziert die Kursbewegung f√ºr einen neuen Reddit Post\n",
    "    \n",
    "    Args:\n",
    "        text: Reddit Post Text\n",
    "        sentiment_finbert: FinBERT Sentiment Score (optional)\n",
    "        sentiment_vader: VADER Sentiment Score (optional)\n",
    "        upvotes: Anzahl Upvotes (default: 1)\n",
    "        model_name: Welches Modell verwenden ('best' oder spezifischer Name)\n",
    "        embedding: Embedding des Textes (falls nicht vorhanden, wird Dummy verwendet)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prognose und Wahrscheinlichkeit\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Bestes Modell ausw√§hlen\n",
    "    if model_name == 'best':\n",
    "        model_name = max(results.keys(), key=lambda x: results[x]['roc_auc'])\n",
    "    \n",
    "    model = results[model_name]['model']\n",
    "    \n",
    "    # Features erstellen\n",
    "    now = datetime.now()\n",
    "    features = []\n",
    "    \n",
    "    # 1. Embedding (Hauptfeature)\n",
    "    if embedding is not None:\n",
    "        features.extend(embedding)\n",
    "    else:\n",
    "        # Dummy Embedding verwenden (Durchschnitt der Trainings-Embeddings)\n",
    "        dummy_embedding = np.mean(train_embeddings, axis=0)\n",
    "        features.extend(dummy_embedding)\n",
    "    \n",
    "    # 2. Sentiment Features (falls vorhanden)\n",
    "    if sentiment_finbert is not None:\n",
    "        features.append(sentiment_finbert)\n",
    "    if sentiment_vader is not None:\n",
    "        features.append(sentiment_vader)\n",
    "    \n",
    "    # 3. Engagement\n",
    "    features.append(upvotes)\n",
    "    \n",
    "    # 4. Text-Length\n",
    "    features.append(len(text))\n",
    "    \n",
    "    # 5. Zeit-Features\n",
    "    features.extend([now.hour, now.weekday()])\n",
    "    \n",
    "    # Features als Array\n",
    "    X_new = np.array(features).reshape(1, -1)\n",
    "    \n",
    "    # Skalierung (falls Logistic Regression)\n",
    "    if model_name == 'Logistic Regression':\n",
    "        X_new = scaler.transform(X_new)\n",
    "    \n",
    "    # Prognose\n",
    "    prediction = model.predict(X_new)[0]\n",
    "    probability = model.predict_proba(X_new)[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'STEIGT' if prediction == 1 else 'F√ÑLLT',\n",
    "        'confidence': probability[prediction],\n",
    "        'probability_steigt': probability[1],\n",
    "        'probability_f√§llt': probability[0],\n",
    "        'model_used': model_name\n",
    "    }\n",
    "\n",
    "# Test der Prediction Function\n",
    "test_prediction = predict_stock_movement(\n",
    "    text=\"Tesla is absolutely killing it! Revolutionary technology!\",\n",
    "    sentiment_finbert=0.8,\n",
    "    sentiment_vader=0.7,\n",
    "    upvotes=50\n",
    ")\n",
    "\n",
    "print(\"\\nüîÆ BEISPIEL-PROGNOSE:\")\n",
    "print(f\"Text: 'Tesla is absolutely killing it! Revolutionary technology!'\")\n",
    "print(f\"Prognose: {test_prediction['prediction']}\")\n",
    "print(f\"Konfidenz: {test_prediction['confidence']:.1%}\")\n",
    "print(f\"Wahrscheinlichkeit Steigt: {test_prediction['probability_steigt']:.1%}\")\n",
    "print(f\"Wahrscheinlichkeit F√§llt: {test_prediction['probability_f√§llt']:.1%}\")\n",
    "print(f\"Verwendetes Modell: {test_prediction['model_used']}\")\n",
    "\n",
    "# Test mit negativem Sentiment\n",
    "test_prediction_neg = predict_stock_movement(\n",
    "    text=\"Tesla is overvalued and will crash soon!\",\n",
    "    sentiment_finbert=-0.6,\n",
    "    sentiment_vader=-0.4,\n",
    "    upvotes=5\n",
    ")\n",
    "\n",
    "print(\"\\nüîÆ NEGATIVES BEISPIEL:\")\n",
    "print(f\"Text: 'Tesla is overvalued and will crash soon!'\")\n",
    "print(f\"Prognose: {test_prediction_neg['prediction']}\")\n",
    "print(f\"Konfidenz: {test_prediction_neg['confidence']:.1%}\")\n",
    "print(f\"Wahrscheinlichkeit Steigt: {test_prediction_neg['probability_steigt']:.1%}\")\n",
    "print(f\"Wahrscheinlichkeit F√§llt: {test_prediction_neg['probability_f√§llt']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311e7d16",
   "metadata": {},
   "source": [
    "## 7. Zusammenfassung und n√§chste Schritte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbe0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ZUSAMMENFASSUNG TESLA STOCK PREDICTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä DATEN:\")\n",
    "print(f\"   ‚Ä¢ Reddit Posts: {len(train_reddit)}\")\n",
    "print(f\"   ‚Ä¢ Embedding Dimension: {train_embeddings.shape[1]}D\")\n",
    "print(f\"   ‚Ä¢ Zus√§tzliche Features: {len(feature_names) - train_embeddings.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Gesamt Features: {X.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Prediction Window: {PREDICTION_DAYS} Tage\")\n",
    "print(f\"   ‚Ä¢ Positive Labels (Kurs steigt): {y.mean():.1%}\")\n",
    "\n",
    "print(f\"\\nüèÜ BESTE PERFORMANCE:\")\n",
    "best_model = max(results.keys(), key=lambda x: results[x]['roc_auc'])\n",
    "print(f\"   ‚Ä¢ Modell: {best_model}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {results[best_model]['accuracy']:.1%}\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC: {results[best_model]['roc_auc']:.3f}\")\n",
    "\n",
    "print(f\"\\nüîß FEATURES IN COMPLET_EMBEDDING:\")\n",
    "print(f\"   ‚Ä¢ Text Embeddings ({train_embeddings.shape[1]}D) - Hauptfeatures\")\n",
    "if 'finbert_sentiment' in reddit_df.columns:\n",
    "    print(f\"   ‚Ä¢ FinBERT Sentiment\")\n",
    "if 'vader_sentiment' in reddit_df.columns:\n",
    "    print(f\"   ‚Ä¢ VADER Sentiment\")\n",
    "if 'upvotes' in reddit_df.columns:\n",
    "    print(f\"   ‚Ä¢ Reddit Upvotes\")\n",
    "elif 'score' in reddit_df.columns:\n",
    "    print(f\"   ‚Ä¢ Reddit Score\")\n",
    "print(f\"   ‚Ä¢ Text-Properties (L√§nge)\")\n",
    "print(f\"   ‚Ä¢ Zeitbasiert (Stunde, Wochentag)\")\n",
    "\n",
    "print(f\"\\nüöÄ N√ÑCHSTE SCHRITTE:\")\n",
    "print(f\"   1. Mehr historische Tesla-Kursdaten f√ºr bessere Labels\")\n",
    "print(f\"   2. Ensemble Methods mit mehreren Modellen\")\n",
    "print(f\"   3. Hyperparameter Tuning (GridSearch)\")\n",
    "print(f\"   4. Cross-Validation f√ºr robustere Evaluation\")\n",
    "print(f\"   5. Feature Selection/Dimensionality Reduction\")\n",
    "print(f\"   6. Real-time Pipeline mit neuen Reddit Posts\")\n",
    "print(f\"   7. Backtesting mit simuliertem Trading\")\n",
    "\n",
    "print(f\"\\n‚úÖ Model erfolgreich mit complet_embedding trainiert!\")\n",
    "print(\"üí° Du kannst jetzt die predict_stock_movement() Funktion verwenden.\")\n",
    "print(\"üéØ Das Modell nutzt die m√§chtigen Embeddings als Hauptfeatures!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
