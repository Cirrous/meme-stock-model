{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dieses Notebook vektorisiert Reddit Posts mit einem verfÃ¼gbaren Embedding Model Ã¼ber die Ollama API.\n",
    "Rate Limit: 12 Requests pro Minute - ABER mit Batch-Processing (mehrere Texte pro Request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama API Client mit Batch Processing fÃ¼r bessere Effizienz\n",
    "class OllamaEmbeddings:\n",
    "    HOST = \"https://f2ki-h100-1.f2.htw-berlin.de\"\n",
    "    PORT = 11435\n",
    "    TIMEOUT = 120\n",
    "    # Rate Limit: 12 Requests pro Minute = 1 Request alle 5 Sekunden\n",
    "    REQUEST_DELAY = 5.0\n",
    "    \n",
    "    @classmethod\n",
    "    def get_embeddings_batch(cls, texts, model=\"mxbai-embed-large:latest\"):\n",
    "        \"\"\"Generiert Embeddings fÃ¼r mehrere Texte in einem Request (BATCH)\"\"\"\n",
    "        url = f\"{cls.HOST}:{cls.PORT}/api/embed\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"accept\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Stelle sicher, dass texts eine Liste von Strings ist\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Bereinige Texte\n",
    "        clean_texts = [str(text).strip() for text in texts if text and str(text).strip()]\n",
    "        \n",
    "        if not clean_texts:\n",
    "            print(\"ERROR: Keine gÃ¼ltigen Texte zum Verarbeiten\")\n",
    "            return None\n",
    "        \n",
    "        # API Parameter fÃ¼r Batch-Processing\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"input\": clean_texts,  # Liste von Texten fÃ¼r Batch-Processing\n",
    "            \"truncate\": True,\n",
    "            \"keep_alive\": \"5m\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=cls.TIMEOUT)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"ERROR: Request failed with status {response.status_code}: {response.text}\")\n",
    "                return None\n",
    "                \n",
    "            result = response.json()\n",
    "            \n",
    "            # API gibt \"embeddings\" Array zurÃ¼ck mit einem Embedding pro Input-Text\n",
    "            embeddings = result.get('embeddings', [])\n",
    "            \n",
    "            if embeddings and len(embeddings) == len(clean_texts):\n",
    "                return [np.array(emb) for emb in embeddings]  # Liste von Embeddings\n",
    "            else:\n",
    "                print(f\"ERROR: Erwartete {len(clean_texts)} Embeddings, erhielt {len(embeddings)}\")\n",
    "                return None\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"ERROR: Timeout nach {cls.TIMEOUT} Sekunden\")\n",
    "            return None\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"ERROR: Verbindungsfehler zum Server\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Unerwarteter Fehler: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_embeddings(cls, text, model=\"mxbai-embed-large:latest\"):\n",
    "        \"\"\"Generiert Embedding fÃ¼r einzelnen Text (Wrapper fÃ¼r KompatibilitÃ¤t)\"\"\"\n",
    "        result = cls.get_embeddings_batch([text], model)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    @classmethod\n",
    "    def wait_for_rate_limit(cls):\n",
    "        \"\"\"Wartet die erforderliche Zeit fÃ¼r Rate Limiting\"\"\"\n",
    "        time.sleep(cls.REQUEST_DELAY)\n",
    "    \n",
    "    @classmethod\n",
    "    def test_connection(cls):\n",
    "        \"\"\"Testet die Verbindung zum Server\"\"\"\n",
    "        try:\n",
    "            test_url = f\"{cls.HOST}:{cls.PORT}/api/tags\"\n",
    "            response = requests.get(test_url, timeout=10)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embedding Model Management ===\n",
      "âœ“ Verwende bekanntes Embedding Model: mxbai-embed-large:latest\n",
      "  - Parameter: 334M\n",
      "  - GrÃ¶ÃŸe: 638.5 MB\n",
      "  - Familie: bert\n",
      "  - Beschreibung: Sehr gut, groÃŸes Model\n",
      "\n",
      "=== Model Test: mxbai-embed-large:latest ===\n",
      "Teste Model mit kurzem Text...\n",
      "âœ“ Model funktioniert perfekt!\n",
      "âœ“ Embedding-Dimension: 1024\n",
      "\n",
      "ðŸŽ‰ BEREIT: Verwende Embedding Model 'mxbai-embed-large:latest'\n",
      "Alle Voraussetzungen erfÃ¼llt - kann mit Vektorisierung beginnen!\n"
     ]
    }
   ],
   "source": [
    "# Embedding Model Management - Direkte Auswahl verfÃ¼gbarer Models\n",
    "print(\"=== Embedding Model Management ===\")\n",
    "\n",
    "# Bekannte verfÃ¼gbare Embedding Models (basierend auf API Response)\n",
    "AVAILABLE_MODELS = {\n",
    "    \"mxbai-embed-large:latest\": {\n",
    "        \"parameter_size\": \"334M\",\n",
    "        \"size_mb\": 638.5,\n",
    "        \"family\": \"bert\",\n",
    "        \"description\": \"Sehr gut, groÃŸes Model\"\n",
    "    },\n",
    "    \"all-minilm:latest\": {\n",
    "        \"parameter_size\": \"23M\", \n",
    "        \"size_mb\": 43.8,\n",
    "        \"family\": \"bert\",\n",
    "        \"description\": \"Schnell, kleines Model\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# WÃ¤hle bevorzugtes Model (mxbai-embed-large fÃ¼r beste QualitÃ¤t)\n",
    "EMBEDDING_MODEL = \"mxbai-embed-large:latest\"\n",
    "MODEL_FOUND = True\n",
    "\n",
    "print(f\"âœ“ Verwende bekanntes Embedding Model: {EMBEDDING_MODEL}\")\n",
    "model_info = AVAILABLE_MODELS[EMBEDDING_MODEL]\n",
    "print(f\"  - Parameter: {model_info['parameter_size']}\")\n",
    "print(f\"  - GrÃ¶ÃŸe: {model_info['size_mb']:.1f} MB\")\n",
    "print(f\"  - Familie: {model_info['family']}\")\n",
    "print(f\"  - Beschreibung: {model_info['description']}\")\n",
    "\n",
    "# Test des Models\n",
    "print(f\"\\n=== Model Test: {EMBEDDING_MODEL} ===\")\n",
    "try:\n",
    "    # Disable SSL warnings\n",
    "    import urllib3\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "    print(\"Teste Model mit kurzem Text...\")\n",
    "    test_embedding = OllamaEmbeddings.get_embeddings(\"Test\", model=EMBEDDING_MODEL)\n",
    "    if test_embedding is not None and len(test_embedding) > 0:\n",
    "        print(f\"âœ“ Model funktioniert perfekt!\")\n",
    "        print(f\"âœ“ Embedding-Dimension: {len(test_embedding)}\")\n",
    "    else:\n",
    "        print(\"âŒ Model antwortet nicht korrekt\")\n",
    "        print(\"Versuche alternatives Model...\")\n",
    "        EMBEDDING_MODEL = \"all-minilm:latest\"\n",
    "        test_embedding = OllamaEmbeddings.get_embeddings(\"Test\", model=EMBEDDING_MODEL)\n",
    "        if test_embedding is not None and len(test_embedding) > 0:\n",
    "            print(f\"âœ“ Alternatives Model funktioniert: {EMBEDDING_MODEL}\")\n",
    "            print(f\"âœ“ Embedding-Dimension: {len(test_embedding)}\")\n",
    "        else:\n",
    "            print(\"âŒ Auch alternatives Model funktioniert nicht\")\n",
    "            MODEL_FOUND = False\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Model-Test fehlgeschlagen: {e}\")\n",
    "    MODEL_FOUND = False\n",
    "\n",
    "# Finaler Status\n",
    "if MODEL_FOUND:\n",
    "    print(f\"\\nðŸŽ‰ BEREIT: Verwende Embedding Model '{EMBEDDING_MODEL}'\")\n",
    "    print(\"Alle Voraussetzungen erfÃ¼llt - kann mit Vektorisierung beginnen!\")\n",
    "else:\n",
    "    print(f\"\\nâŒ FEHLER: Kein funktionierendes Embedding Model verfÃ¼gbar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade CSV Datei...\n",
      "âœ“ Datensatz geladen: 13268 Zeilen\n"
     ]
    }
   ],
   "source": [
    "# Daten laden nur wenn Model verfÃ¼gbar\n",
    "if MODEL_FOUND:\n",
    "    print(\"Lade CSV Datei...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"data/tesla_preprocessed.csv\")\n",
    "        print(f\"âœ“ Datensatz geladen: {len(df)} Zeilen\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der CSV: {e}\")\n",
    "        MODEL_FOUND = False\n",
    "else:\n",
    "    print(\"âš  Ãœberspringe Datenladung - kein funktionierendes Model verfÃ¼gbar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bereite Texte vor...\n",
      "Nach Bereinigung: 13268 Texte\n",
      "ðŸ“Š Batch-Processing Konfiguration:\n",
      "  - Batch-GrÃ¶ÃŸe: 10 Texte pro Request\n",
      "  - Anzahl Batches: 1327\n",
      "  - Texte pro Minute: 120\n",
      "  - GeschÃ¤tzte Zeit: 110.6 Minuten (1.8 Stunden)\n"
     ]
    }
   ],
   "source": [
    "# Text vorbereiten mit Batch-Kalkulation\n",
    "if MODEL_FOUND and 'df' in locals():\n",
    "    print(\"Bereite Texte vor...\")\n",
    "    df['combined_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
    "    df['combined_text'] = df['combined_text'].str.strip()\n",
    "    df = df[df['combined_text'] != '']\n",
    "\n",
    "    print(f\"Nach Bereinigung: {len(df)} Texte\")\n",
    "\n",
    "    # Batch-GrÃ¶ÃŸe fÃ¼r effizientere Verarbeitung\n",
    "    BATCH_SIZE = 10  # 10 Texte pro Request\n",
    "    num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE  # Aufrunden\n",
    "    \n",
    "    # SchÃ¤tze die benÃ¶tigte Zeit mit Batch-Processing\n",
    "    estimated_minutes = (num_batches * OllamaEmbeddings.REQUEST_DELAY) / 60\n",
    "    estimated_hours = estimated_minutes / 60\n",
    "    texts_per_minute = (BATCH_SIZE * 60) / OllamaEmbeddings.REQUEST_DELAY\n",
    "    \n",
    "    print(f\"ðŸ“Š Batch-Processing Konfiguration:\")\n",
    "    print(f\"  - Batch-GrÃ¶ÃŸe: {BATCH_SIZE} Texte pro Request\")\n",
    "    print(f\"  - Anzahl Batches: {num_batches}\")\n",
    "    print(f\"  - Texte pro Minute: {texts_per_minute:.0f}\")\n",
    "    print(f\"  - GeschÃ¤tzte Zeit: {estimated_minutes:.1f} Minuten ({estimated_hours:.1f} Stunden)\")\n",
    "    \n",
    "    # Warnung bei sehr langen Zeiten\n",
    "    if estimated_hours > 2:\n",
    "        print(\"âš  WARNUNG: Lange Verarbeitungszeit!\")\n",
    "        print(\"Optionen:\")\n",
    "        print(\"  - Kleinere Stichprobe: df.head(1000)\")\n",
    "        print(\"  - GrÃ¶ÃŸere Batches (bis zu ~50 je nach TextlÃ¤nge)\")\n",
    "        \n",
    "        # Automatische Reduktion fÃ¼r Demo\n",
    "        if len(df) > 2000:\n",
    "            print(f\"Verwende automatisch erste 1000 Texte fÃ¼r Demo (statt {len(df)})\")\n",
    "            df = df.head(1000)\n",
    "            num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "            estimated_minutes = (num_batches * OllamaEmbeddings.REQUEST_DELAY) / 60\n",
    "            print(f\"Neue geschÃ¤tzte Zeit: {estimated_minutes:.1f} Minuten\")\n",
    "else:\n",
    "    print(\"âš  Ãœberspringe Textvorbereitung - Voraussetzungen nicht erfÃ¼llt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starte Batch-Vektorisierung von 13268 Texten...\n",
      "Model: mxbai-embed-large:latest\n",
      "Batch-GrÃ¶ÃŸe: 10 Texte pro Request\n",
      "Rate Limit: 12 Requests/Minute = 120 Texte/Minute\n",
      "Fortschritt: Batch 5/1327 | Texte: 50/13268 | Rate: 63/min | ETA: 110.2min\n",
      "Fortschritt: Batch 10/1327 | Texte: 100/13268 | Rate: 62/min | ETA: 109.7min\n",
      "Fortschritt: Batch 15/1327 | Texte: 150/13268 | Rate: 61/min | ETA: 109.3min\n",
      "Fortschritt: Batch 20/1327 | Texte: 200/13268 | Rate: 61/min | ETA: 108.9min\n",
      "Fortschritt: Batch 25/1327 | Texte: 250/13268 | Rate: 61/min | ETA: 108.5min\n",
      "Fortschritt: Batch 30/1327 | Texte: 300/13268 | Rate: 61/min | ETA: 108.1min\n",
      "Fortschritt: Batch 35/1327 | Texte: 350/13268 | Rate: 60/min | ETA: 107.7min\n",
      "Fortschritt: Batch 40/1327 | Texte: 400/13268 | Rate: 60/min | ETA: 107.2min\n",
      "Fortschritt: Batch 45/1327 | Texte: 450/13268 | Rate: 60/min | ETA: 106.8min\n",
      "Fortschritt: Batch 50/1327 | Texte: 500/13268 | Rate: 60/min | ETA: 106.4min\n",
      "Fortschritt: Batch 55/1327 | Texte: 550/13268 | Rate: 60/min | ETA: 106.0min\n",
      "Fortschritt: Batch 60/1327 | Texte: 600/13268 | Rate: 60/min | ETA: 105.6min\n",
      "Fortschritt: Batch 65/1327 | Texte: 650/13268 | Rate: 60/min | ETA: 105.2min\n",
      "Fortschritt: Batch 70/1327 | Texte: 700/13268 | Rate: 60/min | ETA: 104.7min\n",
      "Fortschritt: Batch 75/1327 | Texte: 750/13268 | Rate: 60/min | ETA: 104.3min\n",
      "Fortschritt: Batch 80/1327 | Texte: 800/13268 | Rate: 60/min | ETA: 103.9min\n",
      "Fortschritt: Batch 85/1327 | Texte: 850/13268 | Rate: 60/min | ETA: 103.5min\n",
      "Fortschritt: Batch 90/1327 | Texte: 900/13268 | Rate: 60/min | ETA: 103.1min\n",
      "Fortschritt: Batch 95/1327 | Texte: 950/13268 | Rate: 60/min | ETA: 102.7min\n",
      "Fortschritt: Batch 100/1327 | Texte: 1000/13268 | Rate: 60/min | ETA: 102.2min\n",
      "Fortschritt: Batch 105/1327 | Texte: 1050/13268 | Rate: 60/min | ETA: 101.8min\n",
      "Fortschritt: Batch 110/1327 | Texte: 1100/13268 | Rate: 60/min | ETA: 101.4min\n",
      "Fortschritt: Batch 115/1327 | Texte: 1150/13268 | Rate: 60/min | ETA: 101.0min\n",
      "Fortschritt: Batch 120/1327 | Texte: 1200/13268 | Rate: 60/min | ETA: 100.6min\n",
      "Fortschritt: Batch 125/1327 | Texte: 1250/13268 | Rate: 60/min | ETA: 100.2min\n",
      "Fortschritt: Batch 130/1327 | Texte: 1300/13268 | Rate: 60/min | ETA: 99.7min\n",
      "Fortschritt: Batch 135/1327 | Texte: 1350/13268 | Rate: 60/min | ETA: 99.3min\n",
      "Fortschritt: Batch 140/1327 | Texte: 1400/13268 | Rate: 60/min | ETA: 98.9min\n",
      "Fortschritt: Batch 145/1327 | Texte: 1450/13268 | Rate: 60/min | ETA: 98.5min\n",
      "Fortschritt: Batch 150/1327 | Texte: 1500/13268 | Rate: 60/min | ETA: 98.1min\n",
      "Fortschritt: Batch 155/1327 | Texte: 1550/13268 | Rate: 60/min | ETA: 97.7min\n",
      "Fortschritt: Batch 160/1327 | Texte: 1600/13268 | Rate: 60/min | ETA: 97.2min\n",
      "Fortschritt: Batch 165/1327 | Texte: 1650/13268 | Rate: 60/min | ETA: 96.8min\n",
      "Fortschritt: Batch 170/1327 | Texte: 1700/13268 | Rate: 60/min | ETA: 96.4min\n",
      "Fortschritt: Batch 175/1327 | Texte: 1750/13268 | Rate: 60/min | ETA: 96.0min\n",
      "Fortschritt: Batch 180/1327 | Texte: 1800/13268 | Rate: 60/min | ETA: 95.6min\n",
      "Fortschritt: Batch 185/1327 | Texte: 1850/13268 | Rate: 60/min | ETA: 95.2min\n",
      "Fortschritt: Batch 190/1327 | Texte: 1900/13268 | Rate: 60/min | ETA: 94.7min\n",
      "Fortschritt: Batch 195/1327 | Texte: 1950/13268 | Rate: 60/min | ETA: 94.3min\n",
      "Fortschritt: Batch 200/1327 | Texte: 2000/13268 | Rate: 60/min | ETA: 93.9min\n",
      "Fortschritt: Batch 205/1327 | Texte: 2050/13268 | Rate: 60/min | ETA: 93.5min\n",
      "Fortschritt: Batch 210/1327 | Texte: 2100/13268 | Rate: 60/min | ETA: 93.1min\n",
      "Fortschritt: Batch 215/1327 | Texte: 2150/13268 | Rate: 60/min | ETA: 92.7min\n",
      "Fortschritt: Batch 220/1327 | Texte: 2200/13268 | Rate: 60/min | ETA: 92.2min\n",
      "Fortschritt: Batch 225/1327 | Texte: 2250/13268 | Rate: 60/min | ETA: 91.8min\n",
      "Fortschritt: Batch 230/1327 | Texte: 2300/13268 | Rate: 60/min | ETA: 91.4min\n",
      "Fortschritt: Batch 235/1327 | Texte: 2350/13268 | Rate: 60/min | ETA: 91.0min\n",
      "Fortschritt: Batch 240/1327 | Texte: 2400/13268 | Rate: 60/min | ETA: 90.6min\n",
      "Fortschritt: Batch 245/1327 | Texte: 2450/13268 | Rate: 60/min | ETA: 90.2min\n",
      "Fortschritt: Batch 250/1327 | Texte: 2500/13268 | Rate: 60/min | ETA: 89.7min\n",
      "Fortschritt: Batch 255/1327 | Texte: 2550/13268 | Rate: 60/min | ETA: 89.3min\n",
      "Fortschritt: Batch 260/1327 | Texte: 2600/13268 | Rate: 60/min | ETA: 88.9min\n",
      "Fortschritt: Batch 265/1327 | Texte: 2650/13268 | Rate: 60/min | ETA: 88.5min\n",
      "Fortschritt: Batch 270/1327 | Texte: 2700/13268 | Rate: 60/min | ETA: 88.1min\n",
      "Fortschritt: Batch 275/1327 | Texte: 2750/13268 | Rate: 60/min | ETA: 87.7min\n",
      "Fortschritt: Batch 280/1327 | Texte: 2800/13268 | Rate: 60/min | ETA: 87.2min\n",
      "Fortschritt: Batch 285/1327 | Texte: 2850/13268 | Rate: 60/min | ETA: 86.8min\n",
      "Fortschritt: Batch 290/1327 | Texte: 2900/13268 | Rate: 60/min | ETA: 86.4min\n",
      "Fortschritt: Batch 295/1327 | Texte: 2950/13268 | Rate: 60/min | ETA: 86.0min\n",
      "Fortschritt: Batch 300/1327 | Texte: 3000/13268 | Rate: 60/min | ETA: 85.6min\n",
      "Fortschritt: Batch 305/1327 | Texte: 3050/13268 | Rate: 60/min | ETA: 85.2min\n",
      "Fortschritt: Batch 310/1327 | Texte: 3100/13268 | Rate: 60/min | ETA: 84.7min\n",
      "Fortschritt: Batch 315/1327 | Texte: 3150/13268 | Rate: 60/min | ETA: 84.3min\n",
      "Fortschritt: Batch 320/1327 | Texte: 3200/13268 | Rate: 60/min | ETA: 83.9min\n",
      "Fortschritt: Batch 325/1327 | Texte: 3250/13268 | Rate: 60/min | ETA: 83.5min\n",
      "Fortschritt: Batch 330/1327 | Texte: 3300/13268 | Rate: 60/min | ETA: 83.1min\n",
      "Fortschritt: Batch 335/1327 | Texte: 3350/13268 | Rate: 60/min | ETA: 82.7min\n",
      "Fortschritt: Batch 340/1327 | Texte: 3400/13268 | Rate: 60/min | ETA: 82.2min\n",
      "Fortschritt: Batch 345/1327 | Texte: 3450/13268 | Rate: 60/min | ETA: 81.8min\n",
      "Fortschritt: Batch 350/1327 | Texte: 3500/13268 | Rate: 60/min | ETA: 81.4min\n",
      "Fortschritt: Batch 355/1327 | Texte: 3550/13268 | Rate: 60/min | ETA: 81.0min\n",
      "Fortschritt: Batch 360/1327 | Texte: 3600/13268 | Rate: 60/min | ETA: 80.6min\n",
      "Fortschritt: Batch 365/1327 | Texte: 3650/13268 | Rate: 60/min | ETA: 80.2min\n",
      "Fortschritt: Batch 370/1327 | Texte: 3700/13268 | Rate: 60/min | ETA: 79.7min\n",
      "Fortschritt: Batch 375/1327 | Texte: 3750/13268 | Rate: 60/min | ETA: 79.3min\n",
      "Fortschritt: Batch 380/1327 | Texte: 3800/13268 | Rate: 60/min | ETA: 78.9min\n",
      "Fortschritt: Batch 385/1327 | Texte: 3850/13268 | Rate: 60/min | ETA: 78.5min\n",
      "Fortschritt: Batch 390/1327 | Texte: 3900/13268 | Rate: 60/min | ETA: 78.1min\n",
      "Fortschritt: Batch 395/1327 | Texte: 3950/13268 | Rate: 60/min | ETA: 77.7min\n",
      "Fortschritt: Batch 400/1327 | Texte: 4000/13268 | Rate: 60/min | ETA: 77.2min\n",
      "Fortschritt: Batch 405/1327 | Texte: 4050/13268 | Rate: 60/min | ETA: 76.8min\n",
      "Fortschritt: Batch 410/1327 | Texte: 4100/13268 | Rate: 60/min | ETA: 76.4min\n",
      "Fortschritt: Batch 415/1327 | Texte: 4150/13268 | Rate: 60/min | ETA: 76.0min\n",
      "Fortschritt: Batch 420/1327 | Texte: 4200/13268 | Rate: 60/min | ETA: 75.6min\n",
      "Fortschritt: Batch 425/1327 | Texte: 4250/13268 | Rate: 60/min | ETA: 75.2min\n",
      "Fortschritt: Batch 430/1327 | Texte: 4300/13268 | Rate: 60/min | ETA: 74.7min\n",
      "Fortschritt: Batch 435/1327 | Texte: 4350/13268 | Rate: 60/min | ETA: 74.3min\n",
      "Fortschritt: Batch 440/1327 | Texte: 4400/13268 | Rate: 60/min | ETA: 73.9min\n",
      "Fortschritt: Batch 445/1327 | Texte: 4450/13268 | Rate: 60/min | ETA: 73.5min\n",
      "Fortschritt: Batch 450/1327 | Texte: 4500/13268 | Rate: 60/min | ETA: 73.1min\n",
      "Fortschritt: Batch 455/1327 | Texte: 4550/13268 | Rate: 60/min | ETA: 72.7min\n",
      "Fortschritt: Batch 460/1327 | Texte: 4600/13268 | Rate: 60/min | ETA: 72.2min\n",
      "Fortschritt: Batch 465/1327 | Texte: 4650/13268 | Rate: 60/min | ETA: 71.8min\n",
      "Fortschritt: Batch 470/1327 | Texte: 4700/13268 | Rate: 60/min | ETA: 71.4min\n",
      "Fortschritt: Batch 475/1327 | Texte: 4750/13268 | Rate: 60/min | ETA: 71.0min\n",
      "Fortschritt: Batch 480/1327 | Texte: 4800/13268 | Rate: 60/min | ETA: 70.6min\n",
      "Fortschritt: Batch 485/1327 | Texte: 4850/13268 | Rate: 60/min | ETA: 70.2min\n",
      "Fortschritt: Batch 490/1327 | Texte: 4900/13268 | Rate: 60/min | ETA: 69.7min\n",
      "Fortschritt: Batch 495/1327 | Texte: 4950/13268 | Rate: 60/min | ETA: 69.3min\n",
      "Fortschritt: Batch 500/1327 | Texte: 5000/13268 | Rate: 60/min | ETA: 68.9min\n",
      "Fortschritt: Batch 505/1327 | Texte: 5050/13268 | Rate: 60/min | ETA: 68.5min\n",
      "Fortschritt: Batch 510/1327 | Texte: 5100/13268 | Rate: 60/min | ETA: 68.1min\n",
      "Fortschritt: Batch 515/1327 | Texte: 5150/13268 | Rate: 60/min | ETA: 67.7min\n",
      "Fortschritt: Batch 520/1327 | Texte: 5200/13268 | Rate: 60/min | ETA: 67.2min\n",
      "Fortschritt: Batch 525/1327 | Texte: 5250/13268 | Rate: 60/min | ETA: 66.8min\n",
      "Fortschritt: Batch 530/1327 | Texte: 5300/13268 | Rate: 60/min | ETA: 66.4min\n",
      "Fortschritt: Batch 535/1327 | Texte: 5350/13268 | Rate: 60/min | ETA: 66.0min\n",
      "Fortschritt: Batch 540/1327 | Texte: 5400/13268 | Rate: 60/min | ETA: 65.6min\n",
      "Fortschritt: Batch 545/1327 | Texte: 5450/13268 | Rate: 60/min | ETA: 65.2min\n",
      "Fortschritt: Batch 550/1327 | Texte: 5500/13268 | Rate: 60/min | ETA: 64.7min\n",
      "Fortschritt: Batch 555/1327 | Texte: 5550/13268 | Rate: 60/min | ETA: 64.3min\n",
      "Fortschritt: Batch 560/1327 | Texte: 5600/13268 | Rate: 60/min | ETA: 63.9min\n",
      "Fortschritt: Batch 565/1327 | Texte: 5650/13268 | Rate: 60/min | ETA: 63.5min\n",
      "Fortschritt: Batch 570/1327 | Texte: 5700/13268 | Rate: 60/min | ETA: 63.1min\n",
      "Fortschritt: Batch 575/1327 | Texte: 5750/13268 | Rate: 60/min | ETA: 62.6min\n",
      "Fortschritt: Batch 580/1327 | Texte: 5800/13268 | Rate: 60/min | ETA: 62.2min\n",
      "Fortschritt: Batch 585/1327 | Texte: 5850/13268 | Rate: 60/min | ETA: 61.8min\n",
      "Fortschritt: Batch 590/1327 | Texte: 5900/13268 | Rate: 60/min | ETA: 61.4min\n",
      "Fortschritt: Batch 595/1327 | Texte: 5950/13268 | Rate: 60/min | ETA: 61.0min\n",
      "Fortschritt: Batch 600/1327 | Texte: 6000/13268 | Rate: 60/min | ETA: 60.6min\n",
      "Fortschritt: Batch 605/1327 | Texte: 6050/13268 | Rate: 60/min | ETA: 60.1min\n",
      "Fortschritt: Batch 610/1327 | Texte: 6100/13268 | Rate: 60/min | ETA: 59.7min\n",
      "Fortschritt: Batch 615/1327 | Texte: 6150/13268 | Rate: 60/min | ETA: 59.3min\n",
      "Fortschritt: Batch 620/1327 | Texte: 6200/13268 | Rate: 60/min | ETA: 58.9min\n",
      "Fortschritt: Batch 625/1327 | Texte: 6250/13268 | Rate: 60/min | ETA: 58.5min\n",
      "Fortschritt: Batch 630/1327 | Texte: 6300/13268 | Rate: 60/min | ETA: 58.1min\n",
      "Fortschritt: Batch 635/1327 | Texte: 6350/13268 | Rate: 60/min | ETA: 57.6min\n",
      "Fortschritt: Batch 640/1327 | Texte: 6400/13268 | Rate: 60/min | ETA: 57.2min\n",
      "Fortschritt: Batch 645/1327 | Texte: 6450/13268 | Rate: 60/min | ETA: 56.8min\n",
      "Fortschritt: Batch 650/1327 | Texte: 6500/13268 | Rate: 60/min | ETA: 56.4min\n",
      "Fortschritt: Batch 655/1327 | Texte: 6550/13268 | Rate: 60/min | ETA: 56.0min\n",
      "Fortschritt: Batch 660/1327 | Texte: 6600/13268 | Rate: 60/min | ETA: 55.6min\n",
      "Fortschritt: Batch 665/1327 | Texte: 6650/13268 | Rate: 60/min | ETA: 55.1min\n",
      "Fortschritt: Batch 670/1327 | Texte: 6700/13268 | Rate: 60/min | ETA: 54.7min\n",
      "Fortschritt: Batch 675/1327 | Texte: 6750/13268 | Rate: 60/min | ETA: 54.3min\n",
      "Fortschritt: Batch 680/1327 | Texte: 6800/13268 | Rate: 60/min | ETA: 53.9min\n",
      "Fortschritt: Batch 685/1327 | Texte: 6850/13268 | Rate: 60/min | ETA: 53.5min\n",
      "Fortschritt: Batch 690/1327 | Texte: 6900/13268 | Rate: 60/min | ETA: 53.1min\n",
      "Fortschritt: Batch 695/1327 | Texte: 6950/13268 | Rate: 60/min | ETA: 52.6min\n",
      "Fortschritt: Batch 700/1327 | Texte: 7000/13268 | Rate: 60/min | ETA: 52.2min\n",
      "Fortschritt: Batch 705/1327 | Texte: 7050/13268 | Rate: 60/min | ETA: 51.8min\n",
      "Fortschritt: Batch 710/1327 | Texte: 7100/13268 | Rate: 60/min | ETA: 51.4min\n",
      "Fortschritt: Batch 715/1327 | Texte: 7150/13268 | Rate: 60/min | ETA: 51.0min\n",
      "Fortschritt: Batch 720/1327 | Texte: 7200/13268 | Rate: 60/min | ETA: 50.6min\n",
      "Fortschritt: Batch 725/1327 | Texte: 7250/13268 | Rate: 60/min | ETA: 50.1min\n",
      "Fortschritt: Batch 730/1327 | Texte: 7300/13268 | Rate: 60/min | ETA: 49.7min\n",
      "Fortschritt: Batch 735/1327 | Texte: 7350/13268 | Rate: 60/min | ETA: 49.3min\n",
      "Fortschritt: Batch 740/1327 | Texte: 7400/13268 | Rate: 60/min | ETA: 48.9min\n",
      "Fortschritt: Batch 745/1327 | Texte: 7450/13268 | Rate: 60/min | ETA: 48.5min\n",
      "Fortschritt: Batch 750/1327 | Texte: 7500/13268 | Rate: 60/min | ETA: 48.1min\n",
      "Fortschritt: Batch 755/1327 | Texte: 7550/13268 | Rate: 60/min | ETA: 47.6min\n",
      "Fortschritt: Batch 760/1327 | Texte: 7600/13268 | Rate: 60/min | ETA: 47.2min\n",
      "Fortschritt: Batch 765/1327 | Texte: 7650/13268 | Rate: 60/min | ETA: 46.8min\n",
      "Fortschritt: Batch 770/1327 | Texte: 7700/13268 | Rate: 60/min | ETA: 46.4min\n",
      "Fortschritt: Batch 775/1327 | Texte: 7750/13268 | Rate: 60/min | ETA: 46.0min\n",
      "Fortschritt: Batch 780/1327 | Texte: 7800/13268 | Rate: 60/min | ETA: 45.6min\n",
      "Fortschritt: Batch 785/1327 | Texte: 7850/13268 | Rate: 60/min | ETA: 45.1min\n",
      "Fortschritt: Batch 790/1327 | Texte: 7900/13268 | Rate: 60/min | ETA: 44.7min\n",
      "Fortschritt: Batch 795/1327 | Texte: 7950/13268 | Rate: 60/min | ETA: 44.3min\n",
      "Fortschritt: Batch 800/1327 | Texte: 8000/13268 | Rate: 60/min | ETA: 43.9min\n",
      "Fortschritt: Batch 805/1327 | Texte: 8050/13268 | Rate: 60/min | ETA: 43.5min\n",
      "Fortschritt: Batch 810/1327 | Texte: 8100/13268 | Rate: 60/min | ETA: 43.1min\n",
      "Fortschritt: Batch 815/1327 | Texte: 8150/13268 | Rate: 60/min | ETA: 42.6min\n",
      "Fortschritt: Batch 820/1327 | Texte: 8200/13268 | Rate: 60/min | ETA: 42.2min\n",
      "Fortschritt: Batch 825/1327 | Texte: 8250/13268 | Rate: 60/min | ETA: 41.8min\n",
      "Fortschritt: Batch 830/1327 | Texte: 8300/13268 | Rate: 60/min | ETA: 41.4min\n",
      "Fortschritt: Batch 835/1327 | Texte: 8350/13268 | Rate: 60/min | ETA: 41.0min\n",
      "Fortschritt: Batch 840/1327 | Texte: 8400/13268 | Rate: 60/min | ETA: 40.6min\n",
      "Fortschritt: Batch 845/1327 | Texte: 8450/13268 | Rate: 60/min | ETA: 40.1min\n",
      "Fortschritt: Batch 850/1327 | Texte: 8500/13268 | Rate: 60/min | ETA: 39.7min\n",
      "Fortschritt: Batch 855/1327 | Texte: 8550/13268 | Rate: 60/min | ETA: 39.3min\n",
      "Fortschritt: Batch 860/1327 | Texte: 8600/13268 | Rate: 60/min | ETA: 38.9min\n",
      "Fortschritt: Batch 865/1327 | Texte: 8650/13268 | Rate: 60/min | ETA: 38.5min\n",
      "Fortschritt: Batch 870/1327 | Texte: 8700/13268 | Rate: 60/min | ETA: 38.1min\n",
      "Fortschritt: Batch 875/1327 | Texte: 8750/13268 | Rate: 60/min | ETA: 37.6min\n",
      "Fortschritt: Batch 880/1327 | Texte: 8800/13268 | Rate: 60/min | ETA: 37.2min\n",
      "Fortschritt: Batch 885/1327 | Texte: 8850/13268 | Rate: 60/min | ETA: 36.8min\n",
      "Fortschritt: Batch 890/1327 | Texte: 8900/13268 | Rate: 60/min | ETA: 36.4min\n",
      "Fortschritt: Batch 895/1327 | Texte: 8950/13268 | Rate: 60/min | ETA: 36.0min\n",
      "Fortschritt: Batch 900/1327 | Texte: 9000/13268 | Rate: 60/min | ETA: 35.6min\n",
      "Fortschritt: Batch 905/1327 | Texte: 9050/13268 | Rate: 60/min | ETA: 35.1min\n",
      "Fortschritt: Batch 910/1327 | Texte: 9100/13268 | Rate: 60/min | ETA: 34.7min\n",
      "Fortschritt: Batch 915/1327 | Texte: 9150/13268 | Rate: 60/min | ETA: 34.3min\n",
      "Fortschritt: Batch 920/1327 | Texte: 9200/13268 | Rate: 60/min | ETA: 33.9min\n",
      "Fortschritt: Batch 925/1327 | Texte: 9250/13268 | Rate: 60/min | ETA: 33.5min\n",
      "Fortschritt: Batch 930/1327 | Texte: 9300/13268 | Rate: 60/min | ETA: 33.1min\n",
      "Fortschritt: Batch 935/1327 | Texte: 9350/13268 | Rate: 60/min | ETA: 32.6min\n",
      "Fortschritt: Batch 940/1327 | Texte: 9400/13268 | Rate: 60/min | ETA: 32.2min\n",
      "Fortschritt: Batch 945/1327 | Texte: 9450/13268 | Rate: 60/min | ETA: 31.8min\n",
      "Fortschritt: Batch 950/1327 | Texte: 9500/13268 | Rate: 60/min | ETA: 31.4min\n",
      "Fortschritt: Batch 955/1327 | Texte: 9550/13268 | Rate: 60/min | ETA: 31.0min\n",
      "Fortschritt: Batch 960/1327 | Texte: 9600/13268 | Rate: 60/min | ETA: 30.6min\n",
      "Fortschritt: Batch 965/1327 | Texte: 9650/13268 | Rate: 60/min | ETA: 30.1min\n",
      "Fortschritt: Batch 970/1327 | Texte: 9700/13268 | Rate: 60/min | ETA: 29.7min\n",
      "Fortschritt: Batch 975/1327 | Texte: 9750/13268 | Rate: 60/min | ETA: 29.3min\n",
      "Fortschritt: Batch 980/1327 | Texte: 9800/13268 | Rate: 60/min | ETA: 28.9min\n",
      "Fortschritt: Batch 985/1327 | Texte: 9850/13268 | Rate: 60/min | ETA: 28.5min\n",
      "Fortschritt: Batch 990/1327 | Texte: 9900/13268 | Rate: 60/min | ETA: 28.1min\n",
      "Fortschritt: Batch 995/1327 | Texte: 9950/13268 | Rate: 60/min | ETA: 27.6min\n",
      "Fortschritt: Batch 1000/1327 | Texte: 10000/13268 | Rate: 60/min | ETA: 27.2min\n",
      "Fortschritt: Batch 1005/1327 | Texte: 10050/13268 | Rate: 60/min | ETA: 26.8min\n",
      "Fortschritt: Batch 1010/1327 | Texte: 10100/13268 | Rate: 60/min | ETA: 26.4min\n",
      "Fortschritt: Batch 1015/1327 | Texte: 10150/13268 | Rate: 60/min | ETA: 26.0min\n",
      "Fortschritt: Batch 1020/1327 | Texte: 10200/13268 | Rate: 60/min | ETA: 25.6min\n",
      "Fortschritt: Batch 1025/1327 | Texte: 10250/13268 | Rate: 60/min | ETA: 25.1min\n",
      "Fortschritt: Batch 1030/1327 | Texte: 10300/13268 | Rate: 60/min | ETA: 24.7min\n",
      "Fortschritt: Batch 1035/1327 | Texte: 10350/13268 | Rate: 60/min | ETA: 24.3min\n",
      "Fortschritt: Batch 1040/1327 | Texte: 10400/13268 | Rate: 60/min | ETA: 23.9min\n",
      "Fortschritt: Batch 1045/1327 | Texte: 10450/13268 | Rate: 60/min | ETA: 23.5min\n",
      "Fortschritt: Batch 1050/1327 | Texte: 10500/13268 | Rate: 60/min | ETA: 23.1min\n",
      "Fortschritt: Batch 1055/1327 | Texte: 10550/13268 | Rate: 60/min | ETA: 22.6min\n",
      "Fortschritt: Batch 1060/1327 | Texte: 10600/13268 | Rate: 60/min | ETA: 22.2min\n",
      "Fortschritt: Batch 1065/1327 | Texte: 10650/13268 | Rate: 60/min | ETA: 21.8min\n",
      "Fortschritt: Batch 1070/1327 | Texte: 10700/13268 | Rate: 60/min | ETA: 21.4min\n",
      "Fortschritt: Batch 1075/1327 | Texte: 10750/13268 | Rate: 60/min | ETA: 21.0min\n",
      "Fortschritt: Batch 1080/1327 | Texte: 10800/13268 | Rate: 60/min | ETA: 20.6min\n",
      "Fortschritt: Batch 1085/1327 | Texte: 10850/13268 | Rate: 60/min | ETA: 20.1min\n",
      "Fortschritt: Batch 1090/1327 | Texte: 10900/13268 | Rate: 60/min | ETA: 19.7min\n",
      "Fortschritt: Batch 1095/1327 | Texte: 10950/13268 | Rate: 60/min | ETA: 19.3min\n",
      "Fortschritt: Batch 1100/1327 | Texte: 11000/13268 | Rate: 60/min | ETA: 18.9min\n",
      "Fortschritt: Batch 1105/1327 | Texte: 11050/13268 | Rate: 60/min | ETA: 18.5min\n",
      "Fortschritt: Batch 1110/1327 | Texte: 11100/13268 | Rate: 60/min | ETA: 18.1min\n",
      "Fortschritt: Batch 1115/1327 | Texte: 11150/13268 | Rate: 60/min | ETA: 17.6min\n",
      "Fortschritt: Batch 1120/1327 | Texte: 11200/13268 | Rate: 60/min | ETA: 17.2min\n",
      "Fortschritt: Batch 1125/1327 | Texte: 11250/13268 | Rate: 60/min | ETA: 16.8min\n",
      "Fortschritt: Batch 1130/1327 | Texte: 11300/13268 | Rate: 60/min | ETA: 16.4min\n",
      "Fortschritt: Batch 1135/1327 | Texte: 11350/13268 | Rate: 60/min | ETA: 16.0min\n",
      "Fortschritt: Batch 1140/1327 | Texte: 11400/13268 | Rate: 60/min | ETA: 15.6min\n",
      "Fortschritt: Batch 1145/1327 | Texte: 11450/13268 | Rate: 60/min | ETA: 15.2min\n",
      "Fortschritt: Batch 1150/1327 | Texte: 11500/13268 | Rate: 60/min | ETA: 14.7min\n",
      "Fortschritt: Batch 1155/1327 | Texte: 11550/13268 | Rate: 60/min | ETA: 14.3min\n",
      "Fortschritt: Batch 1160/1327 | Texte: 11600/13268 | Rate: 60/min | ETA: 13.9min\n",
      "Fortschritt: Batch 1165/1327 | Texte: 11650/13268 | Rate: 60/min | ETA: 13.5min\n",
      "Fortschritt: Batch 1170/1327 | Texte: 11700/13268 | Rate: 60/min | ETA: 13.1min\n",
      "Fortschritt: Batch 1175/1327 | Texte: 11750/13268 | Rate: 60/min | ETA: 12.7min\n",
      "Fortschritt: Batch 1180/1327 | Texte: 11800/13268 | Rate: 60/min | ETA: 12.2min\n",
      "Fortschritt: Batch 1185/1327 | Texte: 11850/13268 | Rate: 60/min | ETA: 11.8min\n",
      "Fortschritt: Batch 1190/1327 | Texte: 11900/13268 | Rate: 60/min | ETA: 11.4min\n",
      "Fortschritt: Batch 1195/1327 | Texte: 11950/13268 | Rate: 60/min | ETA: 11.0min\n",
      "Fortschritt: Batch 1200/1327 | Texte: 12000/13268 | Rate: 60/min | ETA: 10.6min\n",
      "Fortschritt: Batch 1205/1327 | Texte: 12050/13268 | Rate: 60/min | ETA: 10.2min\n",
      "Fortschritt: Batch 1210/1327 | Texte: 12100/13268 | Rate: 60/min | ETA: 9.7min\n",
      "Fortschritt: Batch 1215/1327 | Texte: 12150/13268 | Rate: 60/min | ETA: 9.3min\n",
      "Fortschritt: Batch 1220/1327 | Texte: 12200/13268 | Rate: 60/min | ETA: 8.9min\n",
      "Fortschritt: Batch 1225/1327 | Texte: 12250/13268 | Rate: 60/min | ETA: 8.5min\n",
      "Fortschritt: Batch 1230/1327 | Texte: 12300/13268 | Rate: 60/min | ETA: 8.1min\n",
      "Fortschritt: Batch 1235/1327 | Texte: 12350/13268 | Rate: 60/min | ETA: 7.7min\n",
      "Fortschritt: Batch 1240/1327 | Texte: 12400/13268 | Rate: 60/min | ETA: 7.2min\n",
      "Fortschritt: Batch 1245/1327 | Texte: 12450/13268 | Rate: 60/min | ETA: 6.8min\n",
      "Fortschritt: Batch 1250/1327 | Texte: 12500/13268 | Rate: 60/min | ETA: 6.4min\n",
      "Fortschritt: Batch 1255/1327 | Texte: 12550/13268 | Rate: 60/min | ETA: 6.0min\n",
      "Fortschritt: Batch 1260/1327 | Texte: 12600/13268 | Rate: 60/min | ETA: 5.6min\n",
      "Fortschritt: Batch 1265/1327 | Texte: 12650/13268 | Rate: 60/min | ETA: 5.2min\n",
      "Fortschritt: Batch 1270/1327 | Texte: 12700/13268 | Rate: 60/min | ETA: 4.7min\n",
      "Fortschritt: Batch 1275/1327 | Texte: 12750/13268 | Rate: 60/min | ETA: 4.3min\n",
      "Fortschritt: Batch 1280/1327 | Texte: 12800/13268 | Rate: 60/min | ETA: 3.9min\n",
      "Fortschritt: Batch 1285/1327 | Texte: 12850/13268 | Rate: 60/min | ETA: 3.5min\n",
      "Fortschritt: Batch 1290/1327 | Texte: 12900/13268 | Rate: 60/min | ETA: 3.1min\n",
      "Fortschritt: Batch 1295/1327 | Texte: 12950/13268 | Rate: 60/min | ETA: 2.6min\n",
      "Fortschritt: Batch 1300/1327 | Texte: 13000/13268 | Rate: 60/min | ETA: 2.2min\n",
      "Fortschritt: Batch 1305/1327 | Texte: 13050/13268 | Rate: 60/min | ETA: 1.8min\n",
      "Fortschritt: Batch 1310/1327 | Texte: 13100/13268 | Rate: 60/min | ETA: 1.4min\n",
      "Fortschritt: Batch 1315/1327 | Texte: 13150/13268 | Rate: 60/min | ETA: 1.0min\n",
      "Fortschritt: Batch 1320/1327 | Texte: 13200/13268 | Rate: 60/min | ETA: 0.6min\n",
      "Fortschritt: Batch 1325/1327 | Texte: 13250/13268 | Rate: 60/min | ETA: 0.1min\n",
      "Fortschritt: Batch 1327/1327 | Texte: 13268/13268 | Rate: 60/min | ETA: 0.0min\n",
      "\n",
      "âœ… Batch-Vektorisierung abgeschlossen!\n",
      "Verarbeitete Texte: 13268\n"
     ]
    }
   ],
   "source": [
    "# Batch-Vektorisierung fÃ¼r deutlich bessere Effizienz\n",
    "if MODEL_FOUND and 'df' in locals():\n",
    "    texts = df['combined_text'].tolist()\n",
    "    \n",
    "    print(f\"ðŸš€ Starte Batch-Vektorisierung von {len(texts)} Texten...\")\n",
    "    print(f\"Model: {EMBEDDING_MODEL}\")\n",
    "    print(f\"Batch-GrÃ¶ÃŸe: {BATCH_SIZE} Texte pro Request\")\n",
    "    print(f\"Rate Limit: 12 Requests/Minute = {texts_per_minute:.0f} Texte/Minute\")\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    all_embeddings = []\n",
    "    failed_indices = []\n",
    "    \n",
    "    # Verarbeite Texte in Batches\n",
    "    for batch_idx in range(0, len(texts), BATCH_SIZE):\n",
    "        batch_end = min(batch_idx + BATCH_SIZE, len(texts))\n",
    "        batch_texts = texts[batch_idx:batch_end]\n",
    "        batch_indices = list(range(batch_idx, batch_end))\n",
    "        \n",
    "        # Rate Limiting: Warte zwischen Requests (auÃŸer beim ersten)\n",
    "        if batch_idx > 0:\n",
    "            OllamaEmbeddings.wait_for_rate_limit()\n",
    "        \n",
    "        # Batch-Embedding generieren\n",
    "        batch_embeddings = OllamaEmbeddings.get_embeddings_batch(batch_texts, model=EMBEDDING_MODEL)\n",
    "        \n",
    "        if batch_embeddings and len(batch_embeddings) == len(batch_texts):\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        else:\n",
    "            print(f\"Fehler bei Batch {batch_idx//BATCH_SIZE + 1}, verwende Null-Vektoren...\")\n",
    "            failed_indices.extend(batch_indices)\n",
    "            \n",
    "            # Fallback: Null-Vektoren fÃ¼r alle Texte im fehlgeschlagenen Batch\n",
    "            for _ in batch_texts:\n",
    "                if all_embeddings:\n",
    "                    null_embedding = np.zeros_like(all_embeddings[0])\n",
    "                else:\n",
    "                    null_embedding = np.zeros(1024)  # Standard-Dimension fÃ¼r mxbai-embed-large\n",
    "                all_embeddings.append(null_embedding)\n",
    "        \n",
    "        # Progress-Update\n",
    "        current_batch = (batch_idx // BATCH_SIZE) + 1\n",
    "        total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        texts_processed = min(batch_end, len(texts))\n",
    "        \n",
    "        if current_batch % 5 == 0 or current_batch == total_batches:  # Alle 5 Batches oder am Ende\n",
    "            elapsed = datetime.now() - start_time\n",
    "            rate = texts_processed / elapsed.total_seconds() * 60  # Texte pro Minute\n",
    "            remaining_texts = len(texts) - texts_processed\n",
    "            eta_seconds = (remaining_texts / texts_per_minute) * 60\n",
    "            eta_minutes = eta_seconds / 60\n",
    "            \n",
    "            print(f\"Fortschritt: Batch {current_batch}/{total_batches} | \"\n",
    "                  f\"Texte: {texts_processed}/{len(texts)} | \"\n",
    "                  f\"Rate: {rate:.0f}/min | ETA: {eta_minutes:.1f}min\")\n",
    "        \n",
    "        # Stopp bei zu vielen aufeinanderfolgenden Fehlern\n",
    "        if len(failed_indices) > 50 and texts_processed < 200:\n",
    "            print(f\"\\nâš  STOPP: Zu viele frÃ¼he Fehler ({len(failed_indices)}) - prÃ¼fe Model und API\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nâœ… Batch-Vektorisierung abgeschlossen!\")\n",
    "    print(f\"Verarbeitete Texte: {len(all_embeddings)}\")\n",
    "    \n",
    "    if failed_indices:\n",
    "        print(f\"âš  Warnung: {len(failed_indices)} Texte konnten nicht vektorisiert werden\")\n",
    "        if len(failed_indices) <= 20:\n",
    "            print(f\"Fehlerhafte Indizes: {failed_indices}\")\n",
    "        else:\n",
    "            print(f\"Erste 20 fehlerhafte Indizes: {failed_indices[:20]}...\")\n",
    "else:\n",
    "    print(\"âš  Ãœberspringe Vektorisierung - Voraussetzungen nicht erfÃ¼llt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Vektorisierung abgeschlossen!\n",
      "Dauer: 3:41:12.633855\n",
      "Embeddings Shape: (13268, 1024)\n",
      "Erfolgreiche Vektorisierungen: 13268/13268\n",
      "Speichere Embeddings...\n",
      "âœ“ Embeddings gespeichert als 'data/Reddit_embeddings_mxbai_embed_large_latest.npy'\n",
      "âœ“ Metadata gespeichert als 'data/embedding_metadata_mxbai_embed_large_latest.pkl'\n",
      "âœ“ Daten gespeichert als 'data/Reddit_metadata_mxbai_embed_large_latest.csv'\n",
      "Embedding-Dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# Embeddings zusammenfÃ¼hren und speichern\n",
    "if 'all_embeddings' in locals() and all_embeddings:\n",
    "    embeddings_array = np.vstack(all_embeddings)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nâœ“ Vektorisierung abgeschlossen!\")\n",
    "    print(f\"Dauer: {duration}\")\n",
    "    print(f\"Embeddings Shape: {embeddings_array.shape}\")\n",
    "    print(f\"Erfolgreiche Vektorisierungen: {len(all_embeddings) - len(failed_indices)}/{len(texts)}\")\n",
    "    \n",
    "    # Speichern mit Model-spezifischen Namen\n",
    "    model_suffix = EMBEDDING_MODEL.replace(':', '_').replace('/', '_').replace('-', '_')\n",
    "    print(\"Speichere Embeddings...\")\n",
    "    np.save(f'data/Reddit_embeddings_{model_suffix}.npy', embeddings_array)\n",
    "    \n",
    "    metadata = {\n",
    "        'model_name': EMBEDDING_MODEL,\n",
    "        'api_host': f\"{OllamaEmbeddings.HOST}:{OllamaEmbeddings.PORT}\",\n",
    "        'embedding_dimension': embeddings_array.shape[1],\n",
    "        'num_texts': embeddings_array.shape[0],\n",
    "        'processing_time': str(duration),\n",
    "        'failed_indices': failed_indices,\n",
    "        'success_rate': (len(all_embeddings) - len(failed_indices)) / len(texts) if 'texts' in locals() else 0,\n",
    "        'rate_limit': '12 requests/minute',\n",
    "        'request_delay': OllamaEmbeddings.REQUEST_DELAY\n",
    "    }\n",
    "    \n",
    "    with open(f'data/embedding_metadata_{model_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    if 'df' in locals():\n",
    "        df_reduced = df[['title','text','score','created']].reset_index(drop=True)\n",
    "        df_reduced.to_csv(f'data/Reddit_metadata_{model_suffix}.csv', index=False)\n",
    "    \n",
    "    print(f\"âœ“ Embeddings gespeichert als 'data/Reddit_embeddings_{model_suffix}.npy'\")\n",
    "    print(f\"âœ“ Metadata gespeichert als 'data/embedding_metadata_{model_suffix}.pkl'\")\n",
    "    print(f\"âœ“ Daten gespeichert als 'data/Reddit_metadata_{model_suffix}.csv'\")\n",
    "    print(f\"Embedding-Dimension: {embeddings_array.shape[1]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"FEHLER: Keine Embeddings generiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding Validierung ===\n",
      "Shape: (13268, 1024)\n",
      "Datentyp: float64\n",
      "Min/Max Werte: -0.1419 / 0.2993\n",
      "Durchschnittliche Norm: 1.0000\n",
      "Null-Vektoren: 0/13268\n",
      "âœ“ Anzahl Null-Vektoren entspricht den fehlgeschlagenen Requests\n"
     ]
    }
   ],
   "source": [
    "# Optionale Validierung der generierten Embeddings\n",
    "if 'embeddings_array' in locals():\n",
    "    print(\"\\n=== Embedding Validierung ===\")\n",
    "    print(f\"Shape: {embeddings_array.shape}\")\n",
    "    print(f\"Datentyp: {embeddings_array.dtype}\")\n",
    "    print(f\"Min/Max Werte: {embeddings_array.min():.4f} / {embeddings_array.max():.4f}\")\n",
    "    print(f\"Durchschnittliche Norm: {np.linalg.norm(embeddings_array, axis=1).mean():.4f}\")\n",
    "    \n",
    "    # PrÃ¼fe auf Null-Vektoren\n",
    "    null_vectors = np.sum(np.all(embeddings_array == 0, axis=1))\n",
    "    print(f\"Null-Vektoren: {null_vectors}/{len(embeddings_array)}\")\n",
    "    \n",
    "    if null_vectors == len(failed_indices):\n",
    "        print(\"âœ“ Anzahl Null-Vektoren entspricht den fehlgeschlagenen Requests\")\n",
    "    else:\n",
    "        print(\"âš  Unerwartete Anzahl an Null-Vektoren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch-GrÃ¶ÃŸen Test ===\n",
      "\n",
      "Teste Batch-GrÃ¶ÃŸe: 1\n",
      "  âœ“ Erfolgreich: 1 Embeddings in 7.09s\n",
      "  âœ“ Rate: 0.1 Texte/Sekunde\n",
      "  âœ“ Embedding Shape: (1024,)\n",
      "\n",
      "Teste Batch-GrÃ¶ÃŸe: 5\n",
      "  âœ“ Erfolgreich: 5 Embeddings in 10.15s\n",
      "  âœ“ Rate: 0.5 Texte/Sekunde\n",
      "  âœ“ Embedding Shape: (1024,)\n",
      "\n",
      "Teste Batch-GrÃ¶ÃŸe: 10\n",
      "  âœ“ Erfolgreich: 10 Embeddings in 10.14s\n",
      "  âœ“ Rate: 1.0 Texte/Sekunde\n",
      "  âœ“ Embedding Shape: (1024,)\n",
      "\n",
      "Teste Batch-GrÃ¶ÃŸe: 20\n",
      "  âœ“ Erfolgreich: 20 Embeddings in 10.28s\n",
      "  âœ“ Rate: 1.9 Texte/Sekunde\n",
      "  âœ“ Embedding Shape: (1024,)\n",
      "\n",
      "Teste Batch-GrÃ¶ÃŸe: 30\n",
      "  âœ“ Erfolgreich: 30 Embeddings in 10.26s\n",
      "  âœ“ Rate: 2.9 Texte/Sekunde\n",
      "  âœ“ Embedding Shape: (1024,)\n",
      "\n",
      "Teste Batch-GrÃ¶ÃŸe: 50\n",
      "  âœ“ Erfolgreich: 50 Embeddings in 10.62s\n",
      "  âœ“ Rate: 4.7 Texte/Sekunde\n",
      "  âœ“ Embedding Shape: (1024,)\n",
      "\n",
      "ðŸ’¡ Test-Ergebnisse fÃ¼r mxbai-embed-large:latest:\n",
      "âœ… Schnellste Batch-GrÃ¶ÃŸe: 50 (4.7 Texte/s)\n",
      "\n",
      "ðŸ“Š Empfohlene Batch-GrÃ¶ÃŸen:\n",
      "â€¢ Empfohlen: 20-30 (ausgewogen)\n",
      "â€¢ Aggressiv: 30-50 (maximale Geschwindigkeit)\n",
      "\n",
      "ðŸš€ Optimierte Batch-GrÃ¶ÃŸe fÃ¼r Produktion: 50\n"
     ]
    }
   ],
   "source": [
    "# Teste verschiedene Batch-GrÃ¶ÃŸen um das Optimum zu finden\n",
    "if MODEL_FOUND and 'df' in locals():\n",
    "    print(\"=== Batch-GrÃ¶ÃŸen Test ===\")\n",
    "    \n",
    "    # PrÃ¼fe ob die Batch-Methode verfÃ¼gbar ist\n",
    "    if not hasattr(OllamaEmbeddings, 'get_embeddings_batch'):\n",
    "        print(\"âŒ FEHLER: get_embeddings_batch Methode nicht gefunden!\")\n",
    "        print(\"ðŸ”§ LÃ–SUNG: Bitte fÃ¼hre Cell 3 (OllamaEmbeddings Klasse) erneut aus\")\n",
    "        print(\"   oder starte den Kernel neu: Kernel â†’ Restart Kernel\")\n",
    "    else:\n",
    "        # Test-Texte (erste 100 fÃ¼r schnellen Test)\n",
    "        test_texts = df['combined_text'].head(100).tolist()\n",
    "        \n",
    "        # Verschiedene Batch-GrÃ¶ÃŸen testen\n",
    "        test_batch_sizes = [1, 5, 10, 20, 30, 50]\n",
    "        successful_batches = []\n",
    "        \n",
    "        for batch_size in test_batch_sizes:\n",
    "            print(f\"\\nTeste Batch-GrÃ¶ÃŸe: {batch_size}\")\n",
    "            \n",
    "            # Nimm erste X Texte fÃ¼r Test\n",
    "            test_batch = test_texts[:batch_size]\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            try:\n",
    "                result = OllamaEmbeddings.get_embeddings_batch(test_batch, model=EMBEDDING_MODEL)\n",
    "                end_time = datetime.now()\n",
    "                duration = (end_time - start_time).total_seconds()\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"  âœ“ Erfolgreich: {len(result)} Embeddings in {duration:.2f}s\")\n",
    "                    print(f\"  âœ“ Rate: {len(result)/duration:.1f} Texte/Sekunde\")\n",
    "                    print(f\"  âœ“ Embedding Shape: {result[0].shape}\")\n",
    "                    successful_batches.append((batch_size, len(result)/duration))\n",
    "                else:\n",
    "                    print(f\"  âŒ Fehlgeschlagen\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Fehler: {e}\")\n",
    "                break\n",
    "        \n",
    "        # Empfehlungen basierend auf erfolgreichen Tests\n",
    "        if successful_batches:\n",
    "            print(f\"\\nðŸ’¡ Test-Ergebnisse fÃ¼r {EMBEDDING_MODEL}:\")\n",
    "            best_batch = max(successful_batches, key=lambda x: x[1])\n",
    "            print(f\"âœ… Schnellste Batch-GrÃ¶ÃŸe: {best_batch[0]} ({best_batch[1]:.1f} Texte/s)\")\n",
    "            \n",
    "            # Empfehlungen\n",
    "            print(f\"\\nðŸ“Š Empfohlene Batch-GrÃ¶ÃŸen:\")\n",
    "            if best_batch[0] <= 5:\n",
    "                print(\"â€¢ Konservativ: 5-10 (sichere Wahl)\")\n",
    "                print(\"â€¢ Empfohlen: 10-15 (ausgewogen)\")\n",
    "            elif best_batch[0] <= 20:\n",
    "                print(\"â€¢ Konservativ: 10-15 (sichere Wahl)\")\n",
    "                print(\"â€¢ Empfohlen: 15-25 (ausgewogen)\")\n",
    "                print(\"â€¢ Aggressiv: 25-40 (maximale Geschwindigkeit)\")\n",
    "            else:\n",
    "                print(\"â€¢ Empfohlen: 20-30 (ausgewogen)\")\n",
    "                print(\"â€¢ Aggressiv: 30-50 (maximale Geschwindigkeit)\")\n",
    "            \n",
    "            # Aktualisiere BATCH_SIZE fÃ¼r optimale Performance\n",
    "            optimal_batch = min(best_batch[0] * 2, 50)  # Doppelt, aber max 50\n",
    "            print(f\"\\nðŸš€ Optimierte Batch-GrÃ¶ÃŸe fÃ¼r Produktion: {optimal_batch}\")\n",
    "            \n",
    "            # Globale Variable fÃ¼r andere Cells setzen\n",
    "            globals()['OPTIMAL_BATCH_SIZE'] = optimal_batch\n",
    "        else:\n",
    "            print(\"\\nâŒ Keine erfolgreichen Batch-Tests\")\n",
    "else:\n",
    "    print(\"âš  Batch-Test Ã¼bersprungen - Voraussetzungen nicht erfÃ¼llt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
