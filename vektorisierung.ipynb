{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Dieses Notebook vektorisiert Reddit Posts mit dem GTE-Multilingual-Base Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU verfügbarkeit prüfen\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model laden\n",
        "print(\"Lade GTE-Multilingual-Base Model...\")\n",
        "model = SentenceTransformer('Alibaba-NLP/gte-multilingual-base', device=device, trust_remote_code=True)\n",
        "print(f\"Model geladen. Embedding Dimension: {model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daten laden\n",
        "print(\"Lade CSV Datei...\")\n",
        "df = pd.read_csv('data/Reddit_Data.csv')\n",
        "print(f\"Datensatz geladen: {len(df)} Zeilen\")\n",
        "print(f\"Spalten: {list(df.columns)}\")\n",
        "print(f\"Erste 3 Zeilen:\")\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text für Embeddings vorbereiten\n",
        "print(\"Bereite Texte vor...\")\n",
        "\n",
        "# Kombiniert Title und Text für bessere Embeddings\n",
        "df['combined_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
        "df['combined_text'] = df['combined_text'].str.strip()\n",
        "\n",
        "# Entfernt leere Texte\n",
        "df = df[df['combined_text'] != '']\n",
        "\n",
        "print(f\"Nach Bereinigung: {len(df)} Texte\")\n",
        "print(f\"Durchschnittliche Textlänge: {df['combined_text'].str.len().mean():.1f} Zeichen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch-Processing Setup\n",
        "BATCH_SIZE = 32  # Anpassbar je nach GPU Memory\n",
        "texts = df['combined_text'].tolist()\n",
        "total_batches = len(texts) // BATCH_SIZE + (1 if len(texts) % BATCH_SIZE else 0)\n",
        "\n",
        "print(f\"Verarbeite {len(texts)} Texte in {total_batches} Batches (Batch Size: {BATCH_SIZE})\")\n",
        "print(f\"Geschätzte Zeit: {total_batches * 2:.1f} - {total_batches * 5:.1f} Minuten\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vektorisierung\n",
        "print(\"Starte Vektorisierung...\")\n",
        "start_time = datetime.now()\n",
        "\n",
        "all_embeddings = []\n",
        "\n",
        "# Process in batches\n",
        "for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Vektorisierung\"):\n",
        "    batch_texts = texts[i:i+BATCH_SIZE]\n",
        "    \n",
        "    # Embeddings für den Batch erstellen\n",
        "    batch_embeddings = model.encode(\n",
        "        batch_texts, \n",
        "        batch_size=BATCH_SIZE,\n",
        "        show_progress_bar=False,\n",
        "        convert_to_numpy=True\n",
        "    )\n",
        "    \n",
        "    all_embeddings.append(batch_embeddings)\n",
        "    \n",
        "    # Memory cleanup alle 100 Batches\n",
        "    if (i // BATCH_SIZE + 1) % 100 == 0:\n",
        "        torch.cuda.empty_cache() if device == 'cuda' else None\n",
        "\n",
        "# Alle Embeddings zusammenführen\n",
        "embeddings_array = np.vstack(all_embeddings)\n",
        "\n",
        "end_time = datetime.now()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(f\"\\nVektorisierung abgeschlossen!\")\n",
        "print(f\"Dauer: {duration}\")\n",
        "print(f\"Embeddings Shape: {embeddings_array.shape}\")\n",
        "print(f\"Durchschnittliche Zeit pro Text: {duration.total_seconds() / len(texts):.4f} Sekunden\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embeddings speichern\n",
        "print(\"Speichere Embeddings...\")\n",
        "\n",
        "# Als NumPy Array speichern\n",
        "np.save('data/Reddit_embeddings.npy', embeddings_array)\n",
        "\n",
        "# Metadaten speichern\n",
        "metadata = {\n",
        "    'model_name': 'Alibaba-NLP/gte-multilingual-base',\n",
        "    'embedding_dimension': embeddings_array.shape[1],\n",
        "    'num_texts': embeddings_array.shape[0],\n",
        "    'processing_time': str(duration),\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'device_used': device\n",
        "}\n",
        "\n",
        "with open('data/embedding_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "# DataFrame mit IDs speichern für spätere Zuordnung\n",
        "df_reduced = df[['title','text','score','created']].reset_index(drop=True)\n",
        "df_reduced.to_csv('data/Reddit_metadata.csv', index=False)\n",
        "\n",
        "print(f\"Embeddings gespeichert als 'data/Reddit_embeddings.npy'\")\n",
        "print(f\"Metadaten gespeichert als 'data/embedding_metadata.pkl'\")\n",
        "print(f\"Post-Metadaten gespeichert als 'data/Reddit_metadata.csv'\")\n",
        "print(f\"\\nDateigröße Embeddings: {os.path.getsize('data/reddit_embeddings.npy') / 1024 / 1024:.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test: Embeddings laden und ähnlichste Posts finden\n",
        "print(\"\\nTest: Lade Embeddings und finde ähnliche Posts...\")\n",
        "\n",
        "# Embeddings laden\n",
        "loaded_embeddings = np.load('data/Reddit_embeddings.npy')\n",
        "loaded_metadata = pd.read_csv('data/Reddit_metadata.csv')\n",
        "\n",
        "print(f\"Geladene Embeddings Shape: {loaded_embeddings.shape}\")\n",
        "\n",
        "# Beispiel: Finde ähnliche Posts zu \"Tesla stock\"\n",
        "query = \"Tesla stock price prediction\"\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Cosine Similarity berechnen\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarities = cosine_similarity(query_embedding, loaded_embeddings)[0]\n",
        "\n",
        "# Top 5 ähnlichste Posts\n",
        "top_indices = np.argsort(similarities)[-5:][::-1]\n",
        "\n",
        "print(f\"\\nTop 5 ähnlichste Posts zu '{query}':\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    print(f\"{i+1}. Similarity: {similarities[idx]:.3f}\")\n",
        "    print(f\"   Subreddit: {loaded_metadata.iloc[idx]['subreddit']}\")\n",
        "    print(f\"   Title: {loaded_metadata.iloc[idx]['title'][:100]}...\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
