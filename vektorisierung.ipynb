{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dieses Notebook vektorisiert Reddit Posts mit einem verf√ºgbaren Embedding Model √ºber die Ollama API.\n",
    "Rate Limit: 12 Requests pro Minute - ABER mit Batch-Processing (mehrere Texte pro Request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama API Client mit Batch Processing f√ºr bessere Effizienz\n",
    "class OllamaEmbeddings:\n",
    "    HOST = \"https://f2ki-h100-1.f2.htw-berlin.de\"\n",
    "    PORT = 11435\n",
    "    TIMEOUT = 120\n",
    "    # Rate Limit: 12 Requests pro Minute = 1 Request alle 5 Sekunden\n",
    "    REQUEST_DELAY = 5.0\n",
    "    \n",
    "    @classmethod\n",
    "    def get_embeddings_batch(cls, texts, model=\"mxbai-embed-large:latest\"):\n",
    "        \"\"\"Generiert Embeddings f√ºr mehrere Texte in einem Request (BATCH)\"\"\"\n",
    "        url = f\"{cls.HOST}:{cls.PORT}/api/embed\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"accept\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Stelle sicher, dass texts eine Liste von Strings ist\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Bereinige Texte\n",
    "        clean_texts = [str(text).strip() for text in texts if text and str(text).strip()]\n",
    "        \n",
    "        if not clean_texts:\n",
    "            print(\"ERROR: Keine g√ºltigen Texte zum Verarbeiten\")\n",
    "            return None\n",
    "        \n",
    "        # API Parameter f√ºr Batch-Processing\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"input\": clean_texts,  # Liste von Texten f√ºr Batch-Processing\n",
    "            \"truncate\": True,\n",
    "            \"keep_alive\": \"5m\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=cls.TIMEOUT)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"ERROR: Request failed with status {response.status_code}: {response.text}\")\n",
    "                return None\n",
    "                \n",
    "            result = response.json()\n",
    "            \n",
    "            # API gibt \"embeddings\" Array zur√ºck mit einem Embedding pro Input-Text\n",
    "            embeddings = result.get('embeddings', [])\n",
    "            \n",
    "            if embeddings and len(embeddings) == len(clean_texts):\n",
    "                return [np.array(emb) for emb in embeddings]  # Liste von Embeddings\n",
    "            else:\n",
    "                print(f\"ERROR: Erwartete {len(clean_texts)} Embeddings, erhielt {len(embeddings)}\")\n",
    "                return None\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"ERROR: Timeout nach {cls.TIMEOUT} Sekunden\")\n",
    "            return None\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"ERROR: Verbindungsfehler zum Server\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Unerwarteter Fehler: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_embeddings(cls, text, model=\"mxbai-embed-large:latest\"):\n",
    "        \"\"\"Generiert Embedding f√ºr einzelnen Text (Wrapper f√ºr Kompatibilit√§t)\"\"\"\n",
    "        result = cls.get_embeddings_batch([text], model)\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    @classmethod\n",
    "    def wait_for_rate_limit(cls):\n",
    "        \"\"\"Wartet die erforderliche Zeit f√ºr Rate Limiting\"\"\"\n",
    "        time.sleep(cls.REQUEST_DELAY)\n",
    "    \n",
    "    @classmethod\n",
    "    def test_connection(cls):\n",
    "        \"\"\"Testet die Verbindung zum Server\"\"\"\n",
    "        try:\n",
    "            test_url = f\"{cls.HOST}:{cls.PORT}/api/tags\"\n",
    "            response = requests.get(test_url, timeout=10)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embedding Model Management ===\n",
      "Pr√ºfe verf√ºgbare Embedding Models...\n",
      "‚ùå Fehler beim Pr√ºfen der Models: HTTPSConnectionPool(host='f2ki-h100-1.f2.htw-berlin.de', port=11435): Read timed out.\n",
      "\n",
      "‚ùå FEHLER: Kein funktionierendes Embedding Model verf√ºgbar!\n",
      "Verf√ºgbare Models auf dem Server:\n",
      "‚ùå Fehler beim Pr√ºfen der Models: HTTPSConnectionPool(host='f2ki-h100-1.f2.htw-berlin.de', port=11435): Read timed out.\n",
      "\n",
      "‚ùå FEHLER: Kein funktionierendes Embedding Model verf√ºgbar!\n",
      "Verf√ºgbare Models auf dem Server:\n",
      "  Konnte Model-Liste nicht abrufen\n",
      "  Konnte Model-Liste nicht abrufen\n"
     ]
    }
   ],
   "source": [
    "# Embedding Model Management - Direkte Auswahl verf√ºgbarer Models\n",
    "print(\"=== Embedding Model Management ===\")\n",
    "\n",
    "# Bekannte verf√ºgbare Embedding Models (basierend auf API Response)\n",
    "AVAILABLE_MODELS = {\n",
    "    \"mxbai-embed-large:latest\": {\n",
    "        \"parameter_size\": \"334M\",\n",
    "        \"size_mb\": 638.5,\n",
    "        \"family\": \"bert\",\n",
    "        \"description\": \"Sehr gut, gro√ües Model\"\n",
    "    },\n",
    "    \"all-minilm:latest\": {\n",
    "        \"parameter_size\": \"23M\", \n",
    "        \"size_mb\": 43.8,\n",
    "        \"family\": \"bert\",\n",
    "        \"description\": \"Schnell, kleines Model\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# W√§hle bevorzugtes Model (mxbai-embed-large f√ºr beste Qualit√§t)\n",
    "EMBEDDING_MODEL = \"mxbai-embed-large:latest\"\n",
    "MODEL_FOUND = True\n",
    "\n",
    "print(f\"‚úì Verwende bekanntes Embedding Model: {EMBEDDING_MODEL}\")\n",
    "model_info = AVAILABLE_MODELS[EMBEDDING_MODEL]\n",
    "print(f\"  - Parameter: {model_info['parameter_size']}\")\n",
    "print(f\"  - Gr√∂√üe: {model_info['size_mb']:.1f} MB\")\n",
    "print(f\"  - Familie: {model_info['family']}\")\n",
    "print(f\"  - Beschreibung: {model_info['description']}\")\n",
    "\n",
    "# Test des Models\n",
    "print(f\"\\n=== Model Test: {EMBEDDING_MODEL} ===\")\n",
    "try:\n",
    "    # Disable SSL warnings\n",
    "    import urllib3\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    \n",
    "    print(\"Teste Model mit kurzem Text...\")\n",
    "    test_embedding = OllamaEmbeddings.get_embeddings(\"Test\", model=EMBEDDING_MODEL)\n",
    "    if test_embedding is not None and len(test_embedding) > 0:\n",
    "        print(f\"‚úì Model funktioniert perfekt!\")\n",
    "        print(f\"‚úì Embedding-Dimension: {len(test_embedding)}\")\n",
    "    else:\n",
    "        print(\"‚ùå Model antwortet nicht korrekt\")\n",
    "        print(\"Versuche alternatives Model...\")\n",
    "        EMBEDDING_MODEL = \"all-minilm:latest\"\n",
    "        test_embedding = OllamaEmbeddings.get_embeddings(\"Test\", model=EMBEDDING_MODEL)\n",
    "        if test_embedding is not None and len(test_embedding) > 0:\n",
    "            print(f\"‚úì Alternatives Model funktioniert: {EMBEDDING_MODEL}\")\n",
    "            print(f\"‚úì Embedding-Dimension: {len(test_embedding)}\")\n",
    "        else:\n",
    "            print(\"‚ùå Auch alternatives Model funktioniert nicht\")\n",
    "            MODEL_FOUND = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model-Test fehlgeschlagen: {e}\")\n",
    "    MODEL_FOUND = False\n",
    "\n",
    "# Finaler Status\n",
    "if MODEL_FOUND:\n",
    "    print(f\"\\nüéâ BEREIT: Verwende Embedding Model '{EMBEDDING_MODEL}'\")\n",
    "    print(\"Alle Voraussetzungen erf√ºllt - kann mit Vektorisierung beginnen!\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå FEHLER: Kein funktionierendes Embedding Model verf√ºgbar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† √úberspringe Datenladung - kein funktionierendes Model verf√ºgbar\n"
     ]
    }
   ],
   "source": [
    "# Daten laden nur wenn Model verf√ºgbar\n",
    "if MODEL_FOUND:\n",
    "    print(\"Lade CSV Datei...\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"data/tesla_preprocessed.csv\")\n",
    "        print(f\"‚úì Datensatz geladen: {len(df)} Zeilen\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden der CSV: {e}\")\n",
    "        MODEL_FOUND = False\n",
    "else:\n",
    "    print(\"‚ö† √úberspringe Datenladung - kein funktionierendes Model verf√ºgbar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† √úberspringe Textvorbereitung - Voraussetzungen nicht erf√ºllt\n"
     ]
    }
   ],
   "source": [
    "# Text vorbereiten mit Batch-Kalkulation\n",
    "if MODEL_FOUND and 'df' in locals():\n",
    "    print(\"Bereite Texte vor...\")\n",
    "    df['combined_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
    "    df['combined_text'] = df['combined_text'].str.strip()\n",
    "    df = df[df['combined_text'] != '']\n",
    "\n",
    "    print(f\"Nach Bereinigung: {len(df)} Texte\")\n",
    "\n",
    "    # Batch-Gr√∂√üe f√ºr effizientere Verarbeitung\n",
    "    BATCH_SIZE = 10  # 10 Texte pro Request\n",
    "    num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE  # Aufrunden\n",
    "    \n",
    "    # Sch√§tze die ben√∂tigte Zeit mit Batch-Processing\n",
    "    estimated_minutes = (num_batches * OllamaEmbeddings.REQUEST_DELAY) / 60\n",
    "    estimated_hours = estimated_minutes / 60\n",
    "    texts_per_minute = (BATCH_SIZE * 60) / OllamaEmbeddings.REQUEST_DELAY\n",
    "    \n",
    "    print(f\"üìä Batch-Processing Konfiguration:\")\n",
    "    print(f\"  - Batch-Gr√∂√üe: {BATCH_SIZE} Texte pro Request\")\n",
    "    print(f\"  - Anzahl Batches: {num_batches}\")\n",
    "    print(f\"  - Texte pro Minute: {texts_per_minute:.0f}\")\n",
    "    print(f\"  - Gesch√§tzte Zeit: {estimated_minutes:.1f} Minuten ({estimated_hours:.1f} Stunden)\")\n",
    "    \n",
    "    # Warnung bei sehr langen Zeiten\n",
    "    if estimated_hours > 2:\n",
    "        print(\"‚ö† WARNUNG: Lange Verarbeitungszeit!\")\n",
    "        print(\"Optionen:\")\n",
    "        print(\"  - Kleinere Stichprobe: df.head(1000)\")\n",
    "        print(\"  - Gr√∂√üere Batches (bis zu ~50 je nach Textl√§nge)\")\n",
    "        \n",
    "        # Automatische Reduktion f√ºr Demo\n",
    "        if len(df) > 2000:\n",
    "            print(f\"Verwende automatisch erste 1000 Texte f√ºr Demo (statt {len(df)})\")\n",
    "            df = df.head(1000)\n",
    "            num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "            estimated_minutes = (num_batches * OllamaEmbeddings.REQUEST_DELAY) / 60\n",
    "            print(f\"Neue gesch√§tzte Zeit: {estimated_minutes:.1f} Minuten\")\n",
    "else:\n",
    "    print(\"‚ö† √úberspringe Textvorbereitung - Voraussetzungen nicht erf√ºllt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† √úberspringe Vektorisierung - Voraussetzungen nicht erf√ºllt\n"
     ]
    }
   ],
   "source": [
    "# Batch-Vektorisierung f√ºr deutlich bessere Effizienz\n",
    "if MODEL_FOUND and 'df' in locals():\n",
    "    texts = df['combined_text'].tolist()\n",
    "    \n",
    "    print(f\"üöÄ Starte Batch-Vektorisierung von {len(texts)} Texten...\")\n",
    "    print(f\"Model: {EMBEDDING_MODEL}\")\n",
    "    print(f\"Batch-Gr√∂√üe: {BATCH_SIZE} Texte pro Request\")\n",
    "    print(f\"Rate Limit: 12 Requests/Minute = {texts_per_minute:.0f} Texte/Minute\")\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    all_embeddings = []\n",
    "    failed_indices = []\n",
    "    \n",
    "    # Verarbeite Texte in Batches\n",
    "    for batch_idx in range(0, len(texts), BATCH_SIZE):\n",
    "        batch_end = min(batch_idx + BATCH_SIZE, len(texts))\n",
    "        batch_texts = texts[batch_idx:batch_end]\n",
    "        batch_indices = list(range(batch_idx, batch_end))\n",
    "        \n",
    "        # Rate Limiting: Warte zwischen Requests (au√üer beim ersten)\n",
    "        if batch_idx > 0:\n",
    "            OllamaEmbeddings.wait_for_rate_limit()\n",
    "        \n",
    "        # Batch-Embedding generieren\n",
    "        batch_embeddings = OllamaEmbeddings.get_embeddings_batch(batch_texts, model=EMBEDDING_MODEL)\n",
    "        \n",
    "        if batch_embeddings and len(batch_embeddings) == len(batch_texts):\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        else:\n",
    "            print(f\"Fehler bei Batch {batch_idx//BATCH_SIZE + 1}, verwende Null-Vektoren...\")\n",
    "            failed_indices.extend(batch_indices)\n",
    "            \n",
    "            # Fallback: Null-Vektoren f√ºr alle Texte im fehlgeschlagenen Batch\n",
    "            for _ in batch_texts:\n",
    "                if all_embeddings:\n",
    "                    null_embedding = np.zeros_like(all_embeddings[0])\n",
    "                else:\n",
    "                    null_embedding = np.zeros(1024)  # Standard-Dimension f√ºr mxbai-embed-large\n",
    "                all_embeddings.append(null_embedding)\n",
    "        \n",
    "        # Progress-Update\n",
    "        current_batch = (batch_idx // BATCH_SIZE) + 1\n",
    "        total_batches = (len(texts) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        texts_processed = min(batch_end, len(texts))\n",
    "        \n",
    "        if current_batch % 5 == 0 or current_batch == total_batches:  # Alle 5 Batches oder am Ende\n",
    "            elapsed = datetime.now() - start_time\n",
    "            rate = texts_processed / elapsed.total_seconds() * 60  # Texte pro Minute\n",
    "            remaining_texts = len(texts) - texts_processed\n",
    "            eta_seconds = (remaining_texts / texts_per_minute) * 60\n",
    "            eta_minutes = eta_seconds / 60\n",
    "            \n",
    "            print(f\"Fortschritt: Batch {current_batch}/{total_batches} | \"\n",
    "                  f\"Texte: {texts_processed}/{len(texts)} | \"\n",
    "                  f\"Rate: {rate:.0f}/min | ETA: {eta_minutes:.1f}min\")\n",
    "        \n",
    "        # Stopp bei zu vielen aufeinanderfolgenden Fehlern\n",
    "        if len(failed_indices) > 50 and texts_processed < 200:\n",
    "            print(f\"\\n‚ö† STOPP: Zu viele fr√ºhe Fehler ({len(failed_indices)}) - pr√ºfe Model und API\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n‚úÖ Batch-Vektorisierung abgeschlossen!\")\n",
    "    print(f\"Verarbeitete Texte: {len(all_embeddings)}\")\n",
    "    \n",
    "    if failed_indices:\n",
    "        print(f\"‚ö† Warnung: {len(failed_indices)} Texte konnten nicht vektorisiert werden\")\n",
    "        if len(failed_indices) <= 20:\n",
    "            print(f\"Fehlerhafte Indizes: {failed_indices}\")\n",
    "        else:\n",
    "            print(f\"Erste 20 fehlerhafte Indizes: {failed_indices[:20]}...\")\n",
    "else:\n",
    "    print(\"‚ö† √úberspringe Vektorisierung - Voraussetzungen nicht erf√ºllt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEHLER: Keine Embeddings generiert!\n"
     ]
    }
   ],
   "source": [
    "# Embeddings zusammenf√ºhren und speichern\n",
    "if 'all_embeddings' in locals() and all_embeddings:\n",
    "    embeddings_array = np.vstack(all_embeddings)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n‚úì Vektorisierung abgeschlossen!\")\n",
    "    print(f\"Dauer: {duration}\")\n",
    "    print(f\"Embeddings Shape: {embeddings_array.shape}\")\n",
    "    print(f\"Erfolgreiche Vektorisierungen: {len(all_embeddings) - len(failed_indices)}/{len(texts)}\")\n",
    "    \n",
    "    # Speichern mit Model-spezifischen Namen\n",
    "    model_suffix = EMBEDDING_MODEL.replace(':', '_').replace('/', '_').replace('-', '_')\n",
    "    print(\"Speichere Embeddings...\")\n",
    "    np.save(f'data/Reddit_embeddings_{model_suffix}.npy', embeddings_array)\n",
    "    \n",
    "    metadata = {\n",
    "        'model_name': EMBEDDING_MODEL,\n",
    "        'api_host': f\"{OllamaEmbeddings.HOST}:{OllamaEmbeddings.PORT}\",\n",
    "        'embedding_dimension': embeddings_array.shape[1],\n",
    "        'num_texts': embeddings_array.shape[0],\n",
    "        'processing_time': str(duration),\n",
    "        'failed_indices': failed_indices,\n",
    "        'success_rate': (len(all_embeddings) - len(failed_indices)) / len(texts) if 'texts' in locals() else 0,\n",
    "        'rate_limit': '12 requests/minute',\n",
    "        'request_delay': OllamaEmbeddings.REQUEST_DELAY\n",
    "    }\n",
    "    \n",
    "    with open(f'data/embedding_metadata_{model_suffix}.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    if 'df' in locals():\n",
    "        df_reduced = df[['title','text','score','created']].reset_index(drop=True)\n",
    "        df_reduced.to_csv(f'data/Reddit_metadata_{model_suffix}.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úì Embeddings gespeichert als 'data/Reddit_embeddings_{model_suffix}.npy'\")\n",
    "    print(f\"‚úì Metadata gespeichert als 'data/embedding_metadata_{model_suffix}.pkl'\")\n",
    "    print(f\"‚úì Daten gespeichert als 'data/Reddit_metadata_{model_suffix}.csv'\")\n",
    "    print(f\"Embedding-Dimension: {embeddings_array.shape[1]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"FEHLER: Keine Embeddings generiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionale Validierung der generierten Embeddings\n",
    "if 'embeddings_array' in locals():\n",
    "    print(\"\\n=== Embedding Validierung ===\")\n",
    "    print(f\"Shape: {embeddings_array.shape}\")\n",
    "    print(f\"Datentyp: {embeddings_array.dtype}\")\n",
    "    print(f\"Min/Max Werte: {embeddings_array.min():.4f} / {embeddings_array.max():.4f}\")\n",
    "    print(f\"Durchschnittliche Norm: {np.linalg.norm(embeddings_array, axis=1).mean():.4f}\")\n",
    "    \n",
    "    # Pr√ºfe auf Null-Vektoren\n",
    "    null_vectors = np.sum(np.all(embeddings_array == 0, axis=1))\n",
    "    print(f\"Null-Vektoren: {null_vectors}/{len(embeddings_array)}\")\n",
    "    \n",
    "    if null_vectors == len(failed_indices):\n",
    "        print(\"‚úì Anzahl Null-Vektoren entspricht den fehlgeschlagenen Requests\")\n",
    "    else:\n",
    "        print(\"‚ö† Unerwartete Anzahl an Null-Vektoren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† Batch-Test √ºbersprungen - Voraussetzungen nicht erf√ºllt\n"
     ]
    }
   ],
   "source": [
    "# Teste verschiedene Batch-Gr√∂√üen um das Optimum zu finden\n",
    "if MODEL_FOUND and 'df' in locals():\n",
    "    print(\"=== Batch-Gr√∂√üen Test ===\")\n",
    "    \n",
    "    # Pr√ºfe ob die Batch-Methode verf√ºgbar ist\n",
    "    if not hasattr(OllamaEmbeddings, 'get_embeddings_batch'):\n",
    "        print(\"‚ùå FEHLER: get_embeddings_batch Methode nicht gefunden!\")\n",
    "        print(\"üîß L√ñSUNG: Bitte f√ºhre Cell 3 (OllamaEmbeddings Klasse) erneut aus\")\n",
    "        print(\"   oder starte den Kernel neu: Kernel ‚Üí Restart Kernel\")\n",
    "    else:\n",
    "        # Test-Texte (erste 100 f√ºr schnellen Test)\n",
    "        test_texts = df['combined_text'].head(100).tolist()\n",
    "        \n",
    "        # Verschiedene Batch-Gr√∂√üen testen\n",
    "        test_batch_sizes = [1, 5, 10, 20, 30, 50]\n",
    "        successful_batches = []\n",
    "        \n",
    "        for batch_size in test_batch_sizes:\n",
    "            print(f\"\\nTeste Batch-Gr√∂√üe: {batch_size}\")\n",
    "            \n",
    "            # Nimm erste X Texte f√ºr Test\n",
    "            test_batch = test_texts[:batch_size]\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            try:\n",
    "                result = OllamaEmbeddings.get_embeddings_batch(test_batch, model=EMBEDDING_MODEL)\n",
    "                end_time = datetime.now()\n",
    "                duration = (end_time - start_time).total_seconds()\n",
    "                \n",
    "                if result:\n",
    "                    print(f\"  ‚úì Erfolgreich: {len(result)} Embeddings in {duration:.2f}s\")\n",
    "                    print(f\"  ‚úì Rate: {len(result)/duration:.1f} Texte/Sekunde\")\n",
    "                    print(f\"  ‚úì Embedding Shape: {result[0].shape}\")\n",
    "                    successful_batches.append((batch_size, len(result)/duration))\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Fehlgeschlagen\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Fehler: {e}\")\n",
    "                break\n",
    "        \n",
    "        # Empfehlungen basierend auf erfolgreichen Tests\n",
    "        if successful_batches:\n",
    "            print(f\"\\nüí° Test-Ergebnisse f√ºr {EMBEDDING_MODEL}:\")\n",
    "            best_batch = max(successful_batches, key=lambda x: x[1])\n",
    "            print(f\"‚úÖ Schnellste Batch-Gr√∂√üe: {best_batch[0]} ({best_batch[1]:.1f} Texte/s)\")\n",
    "            \n",
    "            # Empfehlungen\n",
    "            print(f\"\\nüìä Empfohlene Batch-Gr√∂√üen:\")\n",
    "            if best_batch[0] <= 5:\n",
    "                print(\"‚Ä¢ Konservativ: 5-10 (sichere Wahl)\")\n",
    "                print(\"‚Ä¢ Empfohlen: 10-15 (ausgewogen)\")\n",
    "            elif best_batch[0] <= 20:\n",
    "                print(\"‚Ä¢ Konservativ: 10-15 (sichere Wahl)\")\n",
    "                print(\"‚Ä¢ Empfohlen: 15-25 (ausgewogen)\")\n",
    "                print(\"‚Ä¢ Aggressiv: 25-40 (maximale Geschwindigkeit)\")\n",
    "            else:\n",
    "                print(\"‚Ä¢ Empfohlen: 20-30 (ausgewogen)\")\n",
    "                print(\"‚Ä¢ Aggressiv: 30-50 (maximale Geschwindigkeit)\")\n",
    "            \n",
    "            # Aktualisiere BATCH_SIZE f√ºr optimale Performance\n",
    "            optimal_batch = min(best_batch[0] * 2, 50)  # Doppelt, aber max 50\n",
    "            print(f\"\\nüöÄ Optimierte Batch-Gr√∂√üe f√ºr Produktion: {optimal_batch}\")\n",
    "            \n",
    "            # Globale Variable f√ºr andere Cells setzen\n",
    "            globals()['OPTIMAL_BATCH_SIZE'] = optimal_batch\n",
    "        else:\n",
    "            print(\"\\n‚ùå Keine erfolgreichen Batch-Tests\")\n",
    "else:\n",
    "    print(\"‚ö† Batch-Test √ºbersprungen - Voraussetzungen nicht erf√ºllt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
